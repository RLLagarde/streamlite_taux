# import streamlit as st

# # ---- Gate d‚Äôacc√®s (facultatif mais recommand√©) ----
# gate = st.secrets.get("APP_PASSWORD", "")
# if gate:
#     pw = st.text_input("Mot de passe", type="password")
#     if pw != gate:
#         st.stop()

# from __future__ import annotations
# import os, re, json, imaplib, email, io
# from email.header import decode_header, make_header
# from email.utils import parsedate_to_datetime
# from pathlib import Path
# from datetime import date, datetime, timedelta
# from typing import Dict, List, Tuple

# # (tu as d√©j√† import√© Streamlit ci-dessus)
# from dateutil.relativedelta import relativedelta

# # =========================
# # CONFIG
# # =========================
# st.set_page_config(page_title="Weekly / Monthly ‚Äì Hybrids Views", layout="wide")

# # ‚ö†Ô∏è Ne mets PAS de secrets en dur ; on lit depuis st.secrets
# GMAIL_ADDRESS       = st.secrets.get("GMAIL_ADDRESS", "")
# GMAIL_APP_PASSWORD  = st.secrets.get("GMAIL_APP_PASSWORD", "")
# OPENAI_API_KEY      = st.secrets.get("OPENAI_API_KEY", "")
# OPENAI_MODEL        = st.secrets.get("OPENAI_MODEL", "gpt-4o-mini")

# SUBJECT_WEEKLY_FILTER  = st.secrets.get("SUBJECT_WEEKLY_FILTER",  "Hybrids Financial Debts Views")
# SUBJECT_MONTHLY_FILTER = st.secrets.get("SUBJECT_MONTHLY_FILTER", "Hybrids Financial Debts Views Weekly")

# GMAIL_SEARCH_DAYS = int(st.secrets.get("GMAIL_SEARCH_DAYS", 30))
# MAX_EMAILS_FETCH  = int(st.secrets.get("MAX_EMAILS_FETCH", 400))

# DATA_DIR    = Path("data")
# WEEKLY_DIR  = DATA_DIR / "weekly"
# MONTHLY_DIR = DATA_DIR / "monthly"
# for d in (DATA_DIR, WEEKLY_DIR, MONTHLY_DIR): d.mkdir(parents=True, exist_ok=True)



# # Rubriques (ordre th√©matique)
# DAILY_FIELDS = ["ma_and_ratings","results","financial_credit_spreads","primary_market","other_important_infos"]

# # =========================
# # HELPERS FS/FORMAT
# # =========================
# def weekly_path(w: str) -> Path:   return WEEKLY_DIR / f"{w}.json"
# def monthly_path(m: str) -> Path:  return MONTHLY_DIR / f"{m}.json"

# def load_json(p: Path, default):
#     if p.exists():
#         try: return json.loads(p.read_text(encoding="utf-8"))
#         except: return default
#     return default

# def save_json(p: Path, payload): p.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

# def iso_week_key(d: date) -> str:
#     y, w, _ = d.isocalendar()
#     return f"{y}-W{w:02d}"

# def monday_sunday(d: date) -> Tuple[date, date]:
#     """Retourne le lundi/dimanche de la semaine du jour donn√©."""
#     monday = d - timedelta(days=d.weekday())
#     sunday = monday + timedelta(days=6)
#     return monday, sunday

# def md_block(text: str) -> str:
#     return (text or "").strip().replace("\n", "  \n") or "_(vide)_"

# def normalize_line(s: str) -> str:
#     return re.sub(r"\s+", " ", s.strip().strip("‚Ä¢-‚Äì¬∑* ")).lower()

# def dedupe_bullets(text: str) -> str:
#     seen, out = set(), []
#     for ln in (text or "").splitlines():
#         base = normalize_line(ln)
#         if base and base not in seen:
#             seen.add(base)
#             out.append(f"- {ln.strip().strip('‚Ä¢-‚Äì¬∑* ')}")
#     return "\n".join(out)

# def summarize_texts(texts: List[str], max_bullets=12) -> str:
#     tokens = []
#     for t in texts or []:
#         for ln in str(t or "").splitlines():
#             s = ln.strip(" ‚Ä¢-‚Äì\t ")
#             if s: tokens.append(s)
#     uniq = list(dict.fromkeys(tokens))[:max_bullets]
#     return "\n".join(f"- {b}" for b in uniq)

# # =========================
# # GMAIL
# # =========================
# def _decode_header(s) -> str:
#     try: return str(make_header(decode_header(s or "")))
#     except: return s or ""

# def _strip_html(html: str) -> str:
#     t = re.sub(r"(?is)<(script|style).*?>.*?(</\1>)", " ", html)
#     # IMPORTANT: pas de \n forc√©s sur <br> / </p> -> on laisse le LLM recoller les wraps
#     t = re.sub(r"(?s)<.*?>", " ", t)
#     t = re.sub(r"[ \t]+", " ", t)
#     t = re.sub(r"\n\s*\n\s*\n+", "\n\n", t)
#     return t.strip()

# def _message_to_text(msg: email.message.Message) -> str:
#     if msg.is_multipart():
#         # 1) text/plain prioritaire
#         for part in msg.walk():
#             if part.get_content_type() == "text/plain" and "attachment" not in str(part.get("Content-Disposition") or "").lower():
#                 try: return part.get_payload(decode=True).decode(part.get_content_charset() or "utf-8", errors="replace")
#                 except: pass
#         # 2) text/html
#         for part in msg.walk():
#             if part.get_content_type() == "text/html" and "attachment" not in str(part.get("Content-Disposition") or "").lower():
#                 try: return _strip_html(part.get_payload(decode=True).decode(part.get_content_charset() or "utf-8", errors="replace"))
#                 except: pass
#         # 3) fallback: concat de tous les text/*
#         payloads=[]
#         for part in msg.walk():
#             if part.get_content_maintype()=="text":
#                 try: payloads.append(part.get_payload(decode=True).decode(part.get_content_charset() or "utf-8", errors="replace"))
#                 except: pass
#         return "\n\n".join(payloads)
#     else:
#         try:
#             payload = msg.get_payload(decode=True)
#             if payload is None: return str(msg.get_payload())
#             text = payload.decode(msg.get_content_charset() or "utf-8", errors="replace")
#             return _strip_html(text) if msg.get_content_type()=="text/html" else text
#         except: return str(msg.get_payload())

# def fetch_gmail_messages(newer_than_days=30, limit=400) -> List[dict]:
#     """Renvoie [{subject, body, dt, message_id}] sans filtre d'objet; on filtrera ensuite en Python."""
#     res=[]
#     if not GMAIL_ADDRESS or not GMAIL_APP_PASSWORD:
#         return res
#     try:
#         imap = imaplib.IMAP4_SSL("imap.gmail.com", 993)
#         imap.login(GMAIL_ADDRESS, GMAIL_APP_PASSWORD)
#         ok,_ = imap.select("INBOX")
#         if ok!="OK": raise RuntimeError("INBOX non s√©lectionnable")
#         since = (datetime.utcnow() - timedelta(days=newer_than_days)).strftime("%d-%b-%Y")
#         ok, data = imap.search(None, f'(SINCE {since})')
#         if ok!="OK": return res
#         ids = (data[0].split() or [])[-limit:]
#         for msg_id in reversed(ids):
#             ok, msg_data = imap.fetch(msg_id, "(RFC822)")
#             if ok!="OK": continue
#             msg  = email.message_from_bytes(msg_data[0][1])
#             subj = _decode_header(msg.get("Subject",""))
#             body = _message_to_text(msg)
#             try:
#                 dt = parsedate_to_datetime(msg.get("Date"))
#                 if dt and dt.tzinfo: dt = dt.astimezone().replace(tzinfo=None)
#             except: 
#                 dt = datetime.now()
#             mid = (msg.get("Message-Id") or msg.get("Message-ID") or "").strip() or f"INBOX:{msg_id.decode()}"
#             res.append({"subject": subj, "body": body, "dt": dt, "message_id": mid})
#         imap.logout()
#     except Exception as e:
#         st.error(f"Erreur Gmail IMAP: {e}")
#     return res

# # =========================
# # CLEAN BODY (quotes/signatures)
# # =========================
# QUOTE_PATTERNS = [
#     r"^On .+ wrote:$", r"^Le .+ a √©crit :", r"^From: .+$", r"^-{2,}\s*Original Message\s*-{2,}$",
#     r"^> .*", r"^__+$", r"^\s*De : .+$", r"^\s*Envoy√© : .+$"
# ]
# def strip_quotes_and_signatures(txt: str) -> str:
#     lines, out = (txt or "").splitlines(), []
#     for ln in lines:
#         if any(re.match(p, ln.strip(), re.IGNORECASE) for p in QUOTE_PATTERNS): break
#         out.append(ln)
#     joined = "\n".join(out)
#     joined = re.sub(r"(?is)this message.*?confidential.*", "", joined)
#     joined = re.sub(r"(?is)ce message.*?confidentiel.*", "", joined)
#     return joined.strip()

# # =========================
# # PDF LOADER
# # =========================
# def read_pdf_bytes_to_text(data: bytes) -> str:
#     """Lecture simple PDF -> texte (PyPDF2)."""
#     try:
#         import PyPDF2
#         reader = PyPDF2.PdfReader(io.BytesIO(data))
#         pages = []
#         for p in reader.pages:
#             try:
#                 pages.append(p.extract_text() or "")
#             except:
#                 pages.append("")
#         return "\n".join(pages)
#     except Exception as e:
#         st.error(f"Lecture PDF: {e}")
#         return ""

# def guess_date_from_filename(name: str) -> datetime|None:
#     # essaie YYYY-MM-DD dans le nom de fichier
#     m = re.search(r"(20\d{2})[-_\.]?(0[1-9]|1[0-2])[-_\.]?(0[1-9]|[12]\d|3[01])", name)
#     if not m: return None
#     try:
#         s = f"{m.group(1)}-{m.group(2)}-{m.group(3)}"
#         return datetime.fromisoformat(s)
#     except:
#         return None

# # =========================
# # OPENAI (Classifier + Synth√®ses)
# # =========================
# from openai import OpenAI
# client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# # ---- DAILY CLASSIFIER PROMPT ---- (r√©utilis√© pour chaque document)
# DAILY_SYS = (
#     "Tu es un assistant financier. Classe TOUT le contenu pertinent du document dans EXACTEMENT ces 5 cat√©gories.\n"
#     "Aucune paraphrase. Recopie le texte source tel quel, en phrases compl√®tes (jamais tronqu√©es).\n"
#     "1 item = 1 phrase autonome et grammaticale (pas de virgule finale/parenth√®se ouverte).\n"
#     "Si des retours √† la ligne coupent une phrase, RECOLLE les segments pour former 1 phrase compl√®te.\n"
#     "Exclure signatures/politesse/disclaimers.\n"
#     "En fran√ßais."
# )
# DAILY_USER_TMPL = """Texte :
# ---
# [[EMAIL_BODY]]
# ---
# Retourne un JSON strict:
# {
#   "financial_credit_spreads": ["..."],
#   "primary_market": ["..."],
#   "results": ["..."],
#   "ma_and_ratings": ["..."],
#   "other_important_infos": ["..."]
# }"""

# # ---- WEEKLY SYNTH PROMPT ----
# WEEKLY_SYS = (
#     "Tu es analyste cr√©dit. √Ä partir d‚Äôun contenu agr√©g√© (d√©j√† class√©), g√©n√®re un r√©sum√© hebdomadaire PROPRE.\n"
#     "Objectifs: d√©doublonner, regrouper ce qui se r√©p√®te, garder chiffres/√©ch√©ances exacts, z√©ro hallucination, phrases compl√®tes.\n"
#     "Style: fluide, coh√©rent, structur√©. Ne pas tronquer.\n"
# )
# WEEKLY_USER_TMPL = """Contenu agr√©g√© (par rubrique) :
# ---
# [[WEEKLY_INPUT]]
# ---
# Attendu (JSON strict) :
# {
#   "global_summary": "Quelques lignes par cat√©gories. Ne pas omettre d'informations. Paragraphe structur√©, fluide. Un retour √† la ligne par th√®me (m&a, primary market, results, spreads, autres infos).",
#   "ma_and_ratings": ["2 √† 8 puces propres"],
#   "results": ["2 √† 8 puces propres"],
#   "financial_credit_spreads": ["2 √† 8 puces propres"],
#   "primary_market": ["2 √† 8 puces propres"],
#   "other_important_infos": ["2 √† 8 puces propres"]
# }"""

# # ---- MONTHLY SYNTH PROMPT ----
# MONTHLY_SYS = (
#     "Tu es analyste cr√©dit. √Ä partir de weeklies, produis une synth√®se mensuelle.\n"
#     "Objectifs: tendances du mois, th√®mes dominants, √©v√©nements majeurs, risques et pipeline/primaire.\n"
#     "Conserve chiffres/√©ch√©ances exacts, z√©ro hallucination; phrases compl√®tes.\n"
#     "Style: fluide, coh√©rent, structur√©. Ne pas tronquer. Pas de redondance.\n"
# )
# MONTHLY_USER_TMPL = """Contenu agr√©g√© weeklies (par rubrique) :
# ---
# [[MONTHLY_INPUT]]
# ---
# Attendu (JSON strict) :
# {
#   "global_summary": "6‚Äì16 puces synth√©tiques (th√®mes, drivers, risques, highlights).",
#   "ma_and_ratings": ["2‚Äì10 puces propres"],
#   "results": ["2‚Äì10 puces propres"],
#   "financial_credit_spreads": ["2‚Äì10 puces propres"],
#   "primary_market": ["2‚Äì10 puces propres"],
#   "other_important_infos": ["2‚Äì10 puces propres"]
# }"""

# # --- helpers sortie LLM ---
# def _flatten_spaces(s: str) -> str:
#     return re.sub(r"\s*\n+\s*", " ", str(s or "")).strip()

# def _to_items(value) -> list[str]:
#     if isinstance(value, list):
#         return [_flatten_spaces(x).lstrip("‚Ä¢-‚Äì‚Äî* ").strip() for x in value if str(x).strip()]
#     lines = [ln.rstrip() for ln in str(value or "").splitlines() if ln.strip()]
#     items, cur = [], []
#     for ln in lines:
#         if re.match(r"^\s*[-‚Ä¢‚Äì‚Äî]\s+", ln):
#             if cur: items.append(_flatten_spaces(" ".join(cur)))
#             cur = [ln.lstrip().lstrip("-‚Ä¢‚Äì‚Äî* ").strip()]
#         else:
#             cur.append(ln.strip())
#     if cur: items.append(_flatten_spaces(" ".join(cur)))
#     items = [re.sub(r"\s+", " ", it).strip() for it in items if it and it.strip()]
#     return items

# def classify_with_openai(text: str) -> Dict[str, str]:
#     if not client:
#         return {k:"" for k in DAILY_FIELDS}
#     body = (text or "")[:80000]
#     usr  = DAILY_USER_TMPL.replace("[[EMAIL_BODY]]", body)
#     try:
#         r = client.chat.completions.create(
#             model=OPENAI_MODEL,
#             response_format={"type": "json_object"},
#             temperature=0.1,
#             messages=[{"role":"system","content":DAILY_SYS},{"role":"user","content":usr}],
#         )
#         parsed = json.loads(r.choices[0].message.content or "{}")
#     except Exception as e:
#         st.error(f"OpenAI error (classify): {e}")
#         parsed = {}
#     cleaned = {}
#     for k in DAILY_FIELDS:
#         raw = parsed.get(k, "")
#         items = _to_items(raw)
#         cleaned[k] = "\n".join(items)
#     return cleaned

# def synth_llm_weekly(weekly_input_text: str) -> Dict[str,str]:
#     if not client:
#         return {"global_summary": summarize_texts([weekly_input_text], 12), **{k: "" for k in DAILY_FIELDS}}
#     usr = WEEKLY_USER_TMPL.replace("[[WEEKLY_INPUT]]", weekly_input_text[:80000])
#     try:
#         r = client.chat.completions.create(
#             model=OPENAI_MODEL,
#             response_format={"type":"json_object"},
#             temperature=0.2,
#             messages=[{"role":"system","content":WEEKLY_SYS},{"role":"user","content":usr}],
#         )
#         parsed = json.loads(r.choices[0].message.content or "{}")
#     except Exception as e:
#         st.error(f"OpenAI error (weekly): {e}")
#         parsed = {}
#     out = {"global_summary": _flatten_spaces(parsed.get("global_summary",""))}
#     for k in DAILY_FIELDS:
#         out[k] = "\n".join(_to_items(parsed.get(k,"")))
#     return out

# def synth_llm_monthly(monthly_input_text: str) -> Dict[str,str]:
#     if not client:
#         return {"global_summary": summarize_texts([monthly_input_text], 16), **{k: "" for k in DAILY_FIELDS}}
#     usr = MONTHLY_USER_TMPL.replace("[[MONTHLY_INPUT]]", monthly_input_text[:120000])
#     try:
#         r = client.chat.completions.create(
#             model=OPENAI_MODEL,
#             response_format={"type":"json_object"},
#             temperature=0.2,
#             messages=[{"role":"system","content":MONTHLY_SYS},{"role":"user","content":usr}],
#         )
#         parsed = json.loads(r.choices[0].message.content or "{}")
#     except Exception as e:
#         st.error(f"OpenAI error (monthly): {e}")
#         parsed = {}
#     out = {"global_summary": _flatten_spaces(parsed.get("global_summary",""))}
#     for k in DAILY_FIELDS:
#         out[k] = "\n".join(_to_items(parsed.get(k,"")))
#     return out

# # =========================
# # AGR√âGATION SOURCES
# # =========================
# def search_emails_in_range(start_dt: datetime, end_dt: datetime, subject_filter: str) -> List[dict]:
#     """Filtre les e-mails par fen√™tre de dates + motif d‚Äôobjet (case-insensitive)."""
#     results = []
#     for m in fetch_gmail_messages(newer_than_days=GMAIL_SEARCH_DAYS, limit=MAX_EMAILS_FETCH):
#         if not m.get("dt"): 
#             continue
#         if subject_filter.lower() not in (m["subject"] or "").lower():
#             continue
#         if not (start_dt <= m["dt"] <= end_dt):
#             continue
#         txt = strip_quotes_and_signatures(m["body"])
#         results.append({**m, "body": txt})
#     # tri chrono ascendant
#     results.sort(key=lambda x: x["dt"])
#     return results

# def collect_sources_for_range(
#     start_dt: datetime,
#     end_dt: datetime,
#     subject_filter: str,
#     uploaded_pdfs,
#     emails_override: List[dict] | None = None
# ) -> List[dict]:
#     """
#     R√©cup√®re:
#       - soit la liste d‚Äôe-mails fournie (emails_override),
#       - soit les e-mails IMAP filtr√©s par dates+objet,
#       - et les PDFs d√©pos√©s.
#     Retourne [{kind:'email'|'pdf', dt, title, text}]
#     """
#     docs = []
#     # Emails (override si fourni)
#     emails = emails_override if emails_override is not None else search_emails_in_range(start_dt, end_dt, subject_filter)
#     for m in emails:
#         docs.append({"kind":"email","dt":m["dt"],"title":m["subject"],"text":m["body"]})

#     # PDFs
#     for f in (uploaded_pdfs or []):
#         content = f.read()
#         text = read_pdf_bytes_to_text(content)
#         dt_guess = guess_date_from_filename(f.name) or datetime.now()
#         docs.append({"kind":"pdf","dt":dt_guess,"title":f.name,"text":text})

#     docs.sort(key=lambda x: x["dt"])  # chrono ascendant (plus ancien en haut)
#     return docs

# def classify_docs_to_fields(docs: List[dict]) -> Dict[str, List[Tuple[datetime, str]]]:
#     """
#     Retourne un dict rubrique -> liste [(dt, item1), (dt, item2), ...], tri√© par dt asc.
#     """
#     per_field = {k: [] for k in DAILY_FIELDS}
#     for d in docs:
#         if not d["text"].strip():
#             continue
#         buckets = classify_with_openai(d["text"])
#         # pour chaque rubrique, s√©parer les items (1 item = 1 ligne), et attacher la date
#         for k in DAILY_FIELDS:
#             raw = buckets.get(k, "") or ""
#             for ln in [x.strip() for x in raw.splitlines() if x.strip()]:
#                 per_field[k].append((d["dt"], ln))
#     # ordonner chrono asc par rubrique
#     for k in DAILY_FIELDS:
#         per_field[k].sort(key=lambda t: t[0])
#     return per_field

# def join_fields_as_input(per_field: Dict[str, List[Tuple[datetime,str]]]) -> str:
#     """
#     Construit le bloc d‚Äôentr√©e pour la synth√®se LLM, ordonn√© par th√®me puis chrono (asc).
#     """
#     blocks = []
#     for k in DAILY_FIELDS:
#         items = per_field.get(k, [])
#         if not items: 
#             continue
#         lines = [f"- {txt}" for _, txt in items]
#         blocks.append(f"## {k}\n" + "\n".join(lines))
#     return "\n\n".join(blocks)

# # =========================
# # UI
# # =========================
# st.sidebar.title("Menu")
# page = st.sidebar.radio("Cat√©gories", ["Weekly","Monthly"], index=0)
# st.sidebar.caption("üíæ JSON dans ./data/")

# st.title("üóûÔ∏è Hybrids Financial Debts Views ‚Äî Weekly / Monthly")

# # ---- WEEKLY ----
# if page=="Weekly":
#     st.subheader("Weekly")

#     # Semaine (on choisit un jour -> on calcule lundi/dimanche)
#     chosen_day = st.date_input("Choisir un jour de la semaine", value=date.today(), format="YYYY-MM-DD")
#     monday, sunday = monday_sunday(chosen_day)
#     st.caption(f"Semaine: {monday.isoformat()} ‚Üí {sunday.isoformat()}  (th√®mes puis chrono du plus ancien au plus r√©cent)")

#     # Browse e-mails (semaine)
#     start_dt = datetime.combine(monday, datetime.min.time())
#     end_dt   = datetime.combine(sunday, datetime.max.time())

#     colA, colB = st.columns([1,1])
#     with colA:
#         if st.button("üîç Parcourir les e-mails (semaine)"):
#             emails = search_emails_in_range(start_dt, end_dt, SUBJECT_WEEKLY_FILTER)
#             st.session_state["weekly_email_list"] = emails
#             st.session_state["weekly_selected_ids"] = {m["message_id"] for m in emails}
#     with colB:
#         st.caption("Filtre objet: " + SUBJECT_WEEKLY_FILTER)

#     if "weekly_email_list" in st.session_state:
#         with st.expander("üì¨ E-mails trouv√©s (cliquer pour voir / cocher √† inclure)", expanded=True):
#             sel = st.session_state.get("weekly_selected_ids", set())
#             new_sel = set(sel)
#             for m in st.session_state["weekly_email_list"]:
#                 mid = m["message_id"]
#                 checked = st.checkbox(
#                     f"[{m['dt'].strftime('%Y-%m-%d %H:%M')}] {m['subject']}",
#                     key=f"wk_mail_{mid}",
#                     value=(mid in sel)
#                 )
#                 if checked: new_sel.add(mid)
#                 else:       new_sel.discard(mid)
#                 with st.expander("Voir l‚Äôaper√ßu", expanded=False):
#                     st.write((m["body"] or "")[:1500] + ("..." if len(m["body"])>1500 else ""))
#             st.session_state["weekly_selected_ids"] = new_sel

#     # Choix affichage rubriques
#     st.markdown("**Rubriques √† afficher**")
#     cols = st.columns(5); chosen=[]
#     for i,f in enumerate(DAILY_FIELDS):
#         with cols[i%5]:
#             if st.checkbox(f.replace('_',' ').title(), value=True, key=f"w_{f}"): chosen.append(f)
#     chosen = chosen or DAILY_FIELDS

# # --- Upload PDFs (optionnel si tu utilises des emails) ---
# st.markdown("**D√©posez des e-mails/notes au format PDF (optionnel si vous utilisez les e-mails)**")
# pdf_files = st.file_uploader("Glisser-d√©poser ici", type=["pdf"], accept_multiple_files=True)

# # --- G√©n√©rer ---
# if st.button("‚öôÔ∏è G√©n√©rer le Weekly"):
#     # emails s√©lectionn√©s (si l‚Äôutilisateur a cliqu√© sur "Parcourir les e-mails")
#     selected_emails = []
#     if "weekly_email_list" in st.session_state:
#         ids = st.session_state.get("weekly_selected_ids", set())
#         selected_emails = [m for m in st.session_state["weekly_email_list"] if m["message_id"] in ids]

#     pdf_cnt = len(pdf_files or [])

#     # R√®gle : on peut g√©n√©rer si (au moins 1 e-mail s√©lectionn√©) OU (‚â• 5 PDFs).
#     if not selected_emails and pdf_cnt < 5:
#         st.error("S√©lectionnez au moins **un e-mail** OU d√©posez **‚â• 5 PDFs** pour g√©n√©rer le Weekly.")
#     else:
#         # Fen√™tre temporelle de la semaine
#         start_dt = datetime.combine(monday, datetime.min.time())
#         end_dt   = datetime.combine(sunday, datetime.max.time())

#         # Si on a des emails s√©lectionn√©s, on les passe en override. Sinon on n‚Äôutilise que les PDFs.
#         emails_override = selected_emails if selected_emails else []

#         docs = collect_sources_for_range(
#             start_dt, end_dt,
#             SUBJECT_WEEKLY_FILTER,
#             pdf_files,
#             emails_override=emails_override
#         )

#         if not docs:
#             st.warning("Aucune source s√©lectionn√©e/trouv√©e (v√©rifie la s√©lection d‚Äôe-mails ou tes PDFs).")
#         else:
#             per_field = classify_docs_to_fields(docs)
#             weekly_input = join_fields_as_input(per_field)
#             llm = synth_llm_weekly(weekly_input)

#             fields = {k: dedupe_bullets(llm.get(k,"")) for k in DAILY_FIELDS}
#             global_summary = dedupe_bullets(llm.get("global_summary",""))
#             week_key = iso_week_key(monday)

#             payload = {
#                 "week": week_key,
#                 "window": [monday.isoformat(), sunday.isoformat()],
#                 "fields": fields,
#                 "global_summary": global_summary,
#                 "sources": [{"title":d["title"],"kind":d["kind"],"dt":d["dt"].isoformat()} for d in docs]
#             }
#             st.session_state["weekly_preview"] = payload
#             st.success("Weekly g√©n√©r√©.")

#     if "weekly_preview" in st.session_state:
#         wk = st.session_state["weekly_preview"]
#         st.markdown(f"### Weekly : {wk['week']}")
#         st.write("Fen√™tre :", " ‚Üí ".join(wk.get("window", [])) or "N/A")
#         with st.expander("R√©sum√© global", expanded=True):
#             st.markdown(md_block(wk.get("global_summary","")))
#         for k in DAILY_FIELDS:
#             if k in chosen:
#                 with st.expander(k.replace("_"," ").title(), expanded=False):
#                     st.markdown(md_block(wk["fields"].get(k,"")))
#         with st.expander("Sources utilis√©es"):
#             for s in wk.get("sources", []):
#                 st.write(f"- [{s['kind']}] {s['dt']} ‚Äî {s['title']}")
#         if st.button("üíæ Enregistrer ce Weekly"):
#             save_json(weekly_path(wk['week']), wk); st.success(f"Weekly sauvegard√© : {weekly_path(wk['week'])}")

# # ---- MONTHLY ----
# elif page=="Monthly":
#     st.subheader("Monthly")

#     today = date.today()
#     y = st.number_input("Ann√©e", value=today.year, step=1)
#     m = st.number_input("Mois", 1, 12, value=today.month, step=1)
#     first = date(int(y), int(m), 1)
#     nxt   = first + relativedelta(months=1)
#     st.caption(f"P√©riode: {first.isoformat()} ‚Üí {(nxt - timedelta(days=1)).isoformat()} (th√®mes puis chrono du plus ancien au plus r√©cent)")

#     # Browse e-mails (mois)
#     start_dt = datetime.combine(first, datetime.min.time())
#     end_dt   = datetime.combine(nxt - timedelta(seconds=1), datetime.max.time())

#     colA, colB = st.columns([1,1])
#     with colA:
#         if st.button("üîç Parcourir les e-mails (mois)"):
#             emails = search_emails_in_range(start_dt, end_dt, SUBJECT_MONTHLY_FILTER)
#             st.session_state["monthly_email_list"] = emails
#             st.session_state["monthly_selected_ids"] = {m["message_id"] for m in emails}
#     with colB:
#         st.caption("Filtre objet: " + SUBJECT_MONTHLY_FILTER)

#     if "monthly_email_list" in st.session_state:
#         with st.expander("üì¨ E-mails trouv√©s (cliquer pour voir / cocher √† inclure)", expanded=True):
#             sel = st.session_state.get("monthly_selected_ids", set())
#             new_sel = set(sel)
#             for m in st.session_state["monthly_email_list"]:
#                 mid = m["message_id"]
#                 checked = st.checkbox(
#                     f"[{m['dt'].strftime('%Y-%m-%d %H:%M')}] {m['subject']}",
#                     key=f"mo_mail_{mid}",
#                     value=(mid in sel)
#                 )
#                 if checked: new_sel.add(mid)
#                 else:       new_sel.discard(mid)
#                 with st.expander("Voir l‚Äôaper√ßu", expanded=False):
#                     st.write((m["body"] or "")[:5000] + ("..." if len(m["body"])>5000 else ""))
#             st.session_state["monthly_selected_ids"] = new_sel

#     # Rubriques √† afficher
#     st.markdown("**Rubriques √† afficher**")
#     cols = st.columns(5); chosen=[]
#     for i,f in enumerate(DAILY_FIELDS):
#         with cols[i%5]:
#             if st.checkbox(f.replace('_',' ').title(), value=True, key=f"m_{f}"): chosen.append(f)
#     chosen = chosen or DAILY_FIELDS

#     # Upload PDFs (optionnel)
#     st.markdown("**(Optionnel) D√©poser des PDF**")
#     pdf_files = st.file_uploader("Glisser-d√©poser des PDF (facultatif)", type=["pdf"], accept_multiple_files=True, key="monthly_pdf")

#     if st.button("‚öôÔ∏è G√©n√©rer le Monthly"):
#         emails_override = None
#         if "monthly_email_list" in st.session_state:
#             ids = st.session_state.get("monthly_selected_ids", set())
#             emails_override = [m for m in st.session_state["monthly_email_list"] if m["message_id"] in ids]

#         docs = collect_sources_for_range(start_dt, end_dt, SUBJECT_MONTHLY_FILTER, pdf_files, emails_override=emails_override)
#         if not docs:
#             st.warning("Aucune source s√©lectionn√©e/trouv√©e.")
#         else:
#             per_field = classify_docs_to_fields(docs)
#             monthly_input = join_fields_as_input(per_field)
#             llm = synth_llm_monthly(monthly_input)
#             fields = {k: dedupe_bullets(llm.get(k,"")) for k in DAILY_FIELDS}
#             global_summary = dedupe_bullets(llm.get("global_summary",""))
#             month_key = f"{int(y):04d}-{int(m):02d}"
#             payload = {
#                 "month": month_key,
#                 "fields": fields,
#                 "global_summary": global_summary,
#                 "sources":[{"title":d["title"],"kind":d["kind"],"dt":d["dt"].isoformat()} for d in docs]
#             }
#             st.session_state["monthly_preview"]=payload
#             st.success("Monthly g√©n√©r√©.")

#     if "monthly_preview" in st.session_state:
#         mo = st.session_state["monthly_preview"]
#         st.markdown(f"### Monthly : {mo['month']}")
#         with st.expander("R√©sum√© global", expanded=True):
#             st.markdown(md_block(mo.get("global_summary","")))
#         for k in DAILY_FIELDS:
#             if k in chosen:
#                 with st.expander(k.replace("_"," ").title(), expanded=False):
#                     st.markdown(md_block(mo["fields"].get(k,"")))
#         with st.expander("Sources utilis√©es"):
#             for s in mo.get("sources", []):
#                 st.write(f"- [{s['kind']}] {s['dt']} ‚Äî {s['title']}")
#         if st.button("üíæ Enregistrer ce Monthly"):
#             save_json(monthly_path(mo['month']), mo); st.success(f"Monthly sauvegard√© : {monthly_path(mo['month'])}")



# from __future__ import annotations
# import os, re, json, imaplib, email, io
# from email.header import decode_header, make_header
# from email.utils import parsedate_to_datetime
# from pathlib import Path
# from datetime import date, datetime, timedelta
# from typing import Dict, List, Tuple

# import streamlit as st
# from dateutil.relativedelta import relativedelta

# # =========================
# # GATE (facultatif)
# # =========================
# gate = st.secrets.get("APP_PASSWORD", "")
# if gate:
#     pw = st.text_input("Mot de passe", type="password")
#     if pw != gate:
#         st.stop()

# # =========================
# # CONFIG
# # =========================
# st.set_page_config(page_title="Weekly / Monthly ‚Äì Hybrids Views", layout="wide")

# # ‚ö†Ô∏è Lire les secrets depuis st.secrets (ne pas hardcoder)
# GMAIL_ADDRESS       = st.secrets.get("GMAIL_ADDRESS", "")
# GMAIL_APP_PASSWORD  = st.secrets.get("GMAIL_APP_PASSWORD", "")
# OPENAI_API_KEY      = st.secrets.get("OPENAI_API_KEY", "")
# OPENAI_MODEL        = st.secrets.get("OPENAI_MODEL", "gpt-4o-mini")

# SUBJECT_WEEKLY_FILTER  = st.secrets.get("SUBJECT_WEEKLY_FILTER",  "Hybrids Financial Debts Views")
# SUBJECT_MONTHLY_FILTER = st.secrets.get("SUBJECT_MONTHLY_FILTER", "Hybrids Financial Debts Views Weekly")

# GMAIL_SEARCH_DAYS = int(st.secrets.get("GMAIL_SEARCH_DAYS", 30))
# MAX_EMAILS_FETCH  = int(st.secrets.get("MAX_EMAILS_FETCH", 400))

# DATA_DIR    = Path("data")
# WEEKLY_DIR  = DATA_DIR / "weekly"
# MONTHLY_DIR = DATA_DIR / "monthly"
# for d in (DATA_DIR, WEEKLY_DIR, MONTHLY_DIR): d.mkdir(parents=True, exist_ok=True)

# # Rubriques (ordre th√©matique)
# DAILY_FIELDS = ["ma_and_ratings","results","financial_credit_spreads","primary_market","other_important_infos"]

# # =========================
# # HELPERS FS/FORMAT
# # =========================
# def weekly_path(w: str) -> Path:   return WEEKLY_DIR / f"{w}.json"
# def monthly_path(m: str) -> Path:  return MONTHLY_DIR / f"{m}.json"

# def load_json(p: Path, default):
#     if p.exists():
#         try: return json.loads(p.read_text(encoding="utf-8"))
#         except: return default
#     return default

# def save_json(p: Path, payload): p.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

# def iso_week_key(d: date) -> str:
#     y, w, _ = d.isocalendar()
#     return f"{y}-W{w:02d}"

# def monday_sunday(d: date) -> Tuple[date, date]:
#     monday = d - timedelta(days=d.weekday())
#     sunday = monday + timedelta(days=6)
#     return monday, sunday

# def md_block(text: str) -> str:
#     return (text or "").strip().replace("\n", "  \n") or "_(vide)_"

# def normalize_line(s: str) -> str:
#     return re.sub(r"\s+", " ", s.strip().strip("‚Ä¢-‚Äì¬∑* ")).lower()

# def dedupe_bullets(text: str) -> str:
#     seen, out = set(), []
#     for ln in (text or "").splitlines():
#         base = normalize_line(ln)
#         if base and base not in seen:
#             seen.add(base)
#             out.append(f"- {ln.strip().strip('‚Ä¢-‚Äì¬∑* ')}")
#     return "\n".join(out)

# def summarize_texts(texts: List[str], max_bullets=12) -> str:
#     tokens = []
#     for t in texts or []:
#         for ln in str(t or "").splitlines():
#             s = ln.strip(" ‚Ä¢-‚Äì\t ")
#             if s: tokens.append(s)
#     uniq = list(dict.fromkeys(tokens))[:max_bullets]
#     return "\n".join(f"- {b}" for b in uniq)

# # =========================
# # GMAIL
# # =========================
# def _decode_header(s) -> str:
#     try: return str(make_header(decode_header(s or "")))
#     except: return s or ""

# def _strip_html(html: str) -> str:
#     t = re.sub(r"(?is)<(script|style).*?>.*?(</\1>)", " ", html)
#     t = re.sub(r"(?s)<.*?>", " ", t)
#     t = re.sub(r"[ \t]+", " ", t)
#     t = re.sub(r"\n\s*\n\s*\n+", "\n\n", t)
#     return t.strip()

# def _message_to_text(msg: email.message.Message) -> str:
#     if msg.is_multipart():
#         for part in msg.walk():
#             if part.get_content_type() == "text/plain" and "attachment" not in str(part.get("Content-Disposition") or "").lower():
#                 try: return part.get_payload(decode=True).decode(part.get_content_charset() or "utf-8", errors="replace")
#                 except: pass
#         for part in msg.walk():
#             if part.get_content_type() == "text/html" and "attachment" not in str(part.get("Content-Disposition") or "").lower():
#                 try: return _strip_html(part.get_payload(decode=True).decode(part.get_content_charset() or "utf-8", errors="replace"))
#                 except: pass
#         payloads=[]
#         for part in msg.walk():
#             if part.get_content_maintype()=="text":
#                 try: payloads.append(part.get_payload(decode=True).decode(part.get_content_charset() or "utf-8", errors="replace"))
#                 except: pass
#         return "\n\n".join(payloads)
#     else:
#         try:
#             payload = msg.get_payload(decode=True)
#             if payload is None: return str(msg.get_payload())
#             text = payload.decode(msg.get_content_charset() or "utf-8", errors="replace")
#             return _strip_html(text) if msg.get_content_type()=="text/html" else text
#         except: return str(msg.get_payload())

# def fetch_gmail_messages(newer_than_days=30, limit=400) -> List[dict]:
#     res=[]
#     if not GMAIL_ADDRESS or not GMAIL_APP_PASSWORD:
#         return res
#     try:
#         imap = imaplib.IMAP4_SSL("imap.gmail.com", 993)
#         imap.login(GMAIL_ADDRESS, GMAIL_APP_PASSWORD)
#         ok,_ = imap.select("INBOX")
#         if ok!="OK": raise RuntimeError("INBOX non s√©lectionnable")
#         since = (datetime.utcnow() - timedelta(days=newer_than_days)).strftime("%d-%b-%Y")
#         ok, data = imap.search(None, f'(SINCE {since})')
#         if ok!="OK": return res
#         ids = (data[0].split() or [])[-limit:]
#         for msg_id in reversed(ids):
#             ok, msg_data = imap.fetch(msg_id, "(RFC822)")
#             if ok!="OK": continue
#             msg  = email.message_from_bytes(msg_data[0][1])
#             subj = _decode_header(msg.get("Subject",""))
#             body = _message_to_text(msg)
#             try:
#                 dt = parsedate_to_datetime(msg.get("Date"))
#                 if dt and dt.tzinfo: dt = dt.astimezone().replace(tzinfo=None)
#             except:
#                 dt = datetime.now()
#             mid = (msg.get("Message-Id") or msg.get("Message-ID") or "").strip() or f"INBOX:{msg_id.decode()}"
#             res.append({"subject": subj, "body": body, "dt": dt, "message_id": mid})
#         imap.logout()
#     except Exception as e:
#         st.error(f"Erreur Gmail IMAP: {e}")
#     return res

# # =========================
# # CLEAN BODY (quotes/signatures)
# # =========================
# QUOTE_PATTERNS = [
#     r"^On .+ wrote:$", r"^Le .+ a √©crit :", r"^From: .+$", r"^-{2,}\s*Original Message\s*-{2,}$",
#     r"^> .*", r"^__+$", r"^\s*De : .+$", r"^\s*Envoy√© : .+$"
# ]
# def strip_quotes_and_signatures(txt: str) -> str:
#     lines, out = (txt or "").splitlines(), []
#     for ln in lines:
#         if any(re.match(p, ln.strip(), re.IGNORECASE) for p in QUOTE_PATTERNS): break
#         out.append(ln)
#     joined = "\n".join(out)
#     joined = re.sub(r"(?is)this message.*?confidential.*", "", joined)
#     joined = re.sub(r"(?is)ce message.*?confidentiel.*", "", joined)
#     return joined.strip()

# # =========================
# # PDF LOADER
# # =========================
# def read_pdf_bytes_to_text(data: bytes) -> str:
#     try:
#         import PyPDF2
#         reader = PyPDF2.PdfReader(io.BytesIO(data))
#         pages = []
#         for p in reader.pages:
#             try:
#                 pages.append(p.extract_text() or "")
#             except:
#                 pages.append("")
#         return "\n".join(pages)
#     except Exception as e:
#         st.error(f"Lecture PDF: {e}")
#         return ""

# def guess_date_from_filename(name: str) -> datetime|None:
#     m = re.search(r"(20\d{2})[-_\.]?(0[1-9]|1[0-2])[-_\.]?(0[1-9]|[12]\d|3[01])", name)
#     if not m: return None
#     try:
#         s = f"{m.group(1)}-{m.group(2)}-{m.group(3)}"
#         return datetime.fromisoformat(s)
#     except:
#         return None

# # =========================
# # OPENAI (Classifier + Synth√®ses)
# # =========================
# from openai import OpenAI
# client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# DAILY_SYS = (
#     "Tu es un assistant financier. Classe TOUT le contenu pertinent dans EXACTEMENT ces 5 cat√©gories.\n"
#     "Aucune paraphrase. Recopie le texte source tel quel, en phrases compl√®tes (jamais tronqu√©es).\n"
#     "1 item = 1 phrase autonome (pas de virgule finale/parenth√®se ouverte).\n"
#     "Si des retours √† la ligne coupent une phrase, RECOLLE les segments.\n"
#     "IMPORTANT : conserve l‚ÄôORDRE CHRONOLOGIQUE d‚Äôapparition du texte source.\n"
#     "IMPORTANT : pr√©fixe chaque item par la date au format [YYYY-MM-DD] si elle est fournie.\n"
#     "Exclure signatures/politesse/disclaimers.\n"
#     "En fran√ßais."
# )
# DAILY_USER_TMPL = """Texte :
# ---
# [[EMAIL_BODY]]
# ---
# Retourne un JSON strict:
# {
#   "financial_credit_spreads": ["..."],
#   "primary_market": ["..."],
#   "results": ["..."],
#   "ma_and_ratings": ["..."],
#   "other_important_infos": ["..."]
# }"""

# WEEKLY_SYS = (
#     "Tu es analyste cr√©dit. √Ä partir d‚Äôun contenu agr√©g√© (d√©j√† class√©), g√©n√®re un r√©sum√© hebdomadaire PROPRE.\n"
#     "Objectifs: d√©doublonner, regrouper ce qui se r√©p√®te, garder chiffres/√©ch√©ances exacts, z√©ro hallucination, phrases compl√®tes.\n"
#     "Style: fluide, coh√©rent, structur√©. Ne pas tronquer.\n"
# )
# WEEKLY_USER_TMPL = """Contenu agr√©g√© (par rubrique) :
# ---
# [[WEEKLY_INPUT]]
# ---
# Attendu (JSON strict) :
# {
#   "global_summary": "Quelques lignes par cat√©gories. Ne pas omettre d'informations. Paragraphe structur√©, fluide. Un retour √† la ligne par th√®me (m&a, primary market, results, spreads, autres infos).",
#   "ma_and_ratings": ["2 √† 8 puces propres"],
#   "results": ["2 √† 8 puces propres"],
#   "financial_credit_spreads": ["2 √† 8 puces propres"],
#   "primary_market": ["2 √† 8 puces propres"],
#   "other_important_infos": ["2 √† 8 puces propres"]
# }"""

# MONTHLY_SYS = (
#     "Tu es analyste cr√©dit. √Ä partir de weeklies, produis une synth√®se mensuelle.\n"
#     "Objectifs: tendances du mois, th√®mes dominants, √©v√©nements majeurs, risques et pipeline/primaire.\n"
#     "Conserve chiffres/√©ch√©ances exacts, z√©ro hallucination; phrases compl√®tes.\n"
#     "Style: fluide, coh√©rent, structur√©. Ne pas tronquer. Pas de redondance.\n"
# )
# MONTHLY_USER_TMPL = """Contenu agr√©g√© weeklies (par rubrique) :
# ---
# [[MONTHLY_INPUT]]
# ---
# Attendu (JSON strict) :
# {
#   "global_summary": "6‚Äì16 puces synth√©tiques (th√®mes, drivers, risques, highlights).",
#   "ma_and_ratings": ["2‚Äì10 puces propres"],
#   "results": ["2‚Äì10 puces propres"],
#   "financial_credit_spreads": ["2‚Äì10 puces propres"],
#   "primary_market": ["2‚Äì10 puces propres"],
#   "other_important_infos": ["2‚Äì10 puces propres"]
# }"""

# def _flatten_spaces(s: str) -> str:
#     return re.sub(r"\s*\n+\s*", " ", str(s or "")).strip()

# def _to_items(value) -> list[str]:
#     if isinstance(value, list):
#         return [_flatten_spaces(x).lstrip("‚Ä¢-‚Äì‚Äî* ").strip() for x in value if str(x).strip()]
#     lines = [ln.rstrip() for ln in str(value or "").splitlines() if ln.strip()]
#     items, cur = [], []
#     for ln in lines:
#         if re.match(r"^\s*[-‚Ä¢‚Äì‚Äî]\s+", ln):
#             if cur: items.append(_flatten_spaces(" ".join(cur)))
#             cur = [ln.lstrip().lstrip("-‚Ä¢‚Äì‚Äî* ").strip()]
#         else:
#             cur.append(ln.strip())
#     if cur: items.append(_flatten_spaces(" ".join(cur)))
#     items = [re.sub(r"\s+", " ", it).strip() for it in items if it and it.strip()]
#     return items

# def classify_with_openai(text: str) -> Dict[str, str]:
#     if not client:
#         return {k:"" for k in DAILY_FIELDS}
#     body = (text or "")[:80000]
#     usr  = DAILY_USER_TMPL.replace("[[EMAIL_BODY]]", body)
#     try:
#         r = client.chat.completions.create(
#             model=OPENAI_MODEL,
#             response_format={"type": "json_object"},
#             temperature=0.1,
#             messages=[{"role":"system","content":DAILY_SYS},{"role":"user","content":usr}],
#         )
#         parsed = json.loads(r.choices[0].message.content or "{}")
#     except Exception as e:
#         st.error(f"OpenAI error (classify): {e}")
#         parsed = {}
#     cleaned = {}
#     for k in DAILY_FIELDS:
#         raw = parsed.get(k, "")
#         items = _to_items(raw)
#         cleaned[k] = "\n".join(items)
#     return cleaned

# def synth_llm_weekly(weekly_input_text: str) -> Dict[str,str]:
#     if not client:
#         return {"global_summary": summarize_texts([weekly_input_text], 12), **{k: "" for k in DAILY_FIELDS}}
#     usr = WEEKLY_USER_TMPL.replace("[[WEEKLY_INPUT]]", weekly_input_text[:80000])
#     try:
#         r = client.chat.completions.create(
#             model=OPENAI_MODEL,
#             response_format={"type":"json_object"},
#             temperature=0.2,
#             messages=[{"role":"system","content":WEEKLY_SYS},{"role":"user","content":usr}],
#         )
#         parsed = json.loads(r.choices[0].message.content or "{}")
#     except Exception as e:
#         st.error(f"OpenAI error (weekly): {e}")
#         parsed = {}
#     out = {"global_summary": _flatten_spaces(parsed.get("global_summary",""))}
#     for k in DAILY_FIELDS:
#         out[k] = "\n".join(_to_items(parsed.get(k,"")))
#     return out

# def synth_llm_monthly(monthly_input_text: str) -> Dict[str,str]:
#     if not client:
#         return {"global_summary": summarize_texts([monthly_input_text], 16), **{k: "" for k in DAILY_FIELDS}}
#     usr = MONTHLY_USER_TMPL.replace("[[MONTHLY_INPUT]]", monthly_input_text[:120000])
#     try:
#         r = client.chat.completions.create(
#             model=OPENAI_MODEL,
#             response_format={"type":"json_object"},
#             temperature=0.2,
#             messages=[{"role":"system","content":MONTHLY_SYS},{"role":"user","content":usr}],
#         )
#         parsed = json.loads(r.choices[0].message.content or "{}")
#     except Exception as e:
#         st.error(f"OpenAI error (monthly): {e}")
#         parsed = {}
#     out = {"global_summary": _flatten_spaces(parsed.get("global_summary",""))}
#     for k in DAILY_FIELDS:
#         out[k] = "\n".join(_to_items(parsed.get(k,"")))
#     return out

# # =========================
# # ONE-SHOT CLASSIFY HELPERS
# # =========================
# DATE_TAG_RE = re.compile(r"^\s*\[([0-9]{4}-[0-9]{2}-[0-9]{2})\]\s*(.*)")

# def build_aggregate_text(docs: List[dict]) -> str:
#     chunks = []
#     for d in sorted(docs, key=lambda x: x["dt"]):
#         ddate = d["dt"].date().isoformat()
#         title = (d.get("title") or "").strip()
#         text  = (d.get("text")  or "").strip()
#         if not text:
#             continue
#         head = f"[{ddate}] {title}".strip()
#         chunks.append(f"{head}\n{text}")
#     return "\n\n---\n\n".join(chunks)

# def ensure_dash_bullets(s: str) -> str:
#     return "\n".join(f"- {ln.strip().lstrip('- ')}" for ln in (s or "").splitlines() if ln.strip())

# def sort_items_by_date_prefix(text_block: str) -> str:
#     rows = [ln.strip().lstrip("- ").strip() for ln in (text_block or "").splitlines() if ln.strip()]
#     tagged = []
#     for i, row in enumerate(rows):
#         m = DATE_TAG_RE.match(row)
#         if m:
#             try:
#                 dt = datetime.fromisoformat(m.group(1)).date()
#             except:
#                 dt = date.min
#             txt = m.group(2).strip()
#             tagged.append((dt, i, txt))
#         else:
#             tagged.append((date.min, i, row))
#     tagged.sort(key=lambda t: (t[0], t[1]))
#     return "\n".join(f"- {t[2]}" for t in tagged)

# def classify_all_at_once(docs: List[dict]) -> Dict[str, str]:
#     big_text = build_aggregate_text(docs)
#     buckets  = classify_with_openai(big_text)  # UN seul appel
#     out = {}
#     for k in DAILY_FIELDS:
#         value = ensure_dash_bullets(buckets.get(k, ""))
#         value = dedupe_bullets(value)
#         value = sort_items_by_date_prefix(value)
#         out[k] = value
#     return out

# # =========================
# # AGR√âGATION SOURCES
# # =========================
# def search_emails_in_range(start_dt: datetime, end_dt: datetime, subject_filter: str) -> List[dict]:
#     results = []
#     for m in fetch_gmail_messages(newer_than_days=GMAIL_SEARCH_DAYS, limit=MAX_EMAILS_FETCH):
#         if not m.get("dt"): 
#             continue
#         if subject_filter.lower() not in (m["subject"] or "").lower():
#             continue
#         if not (start_dt <= m["dt"] <= end_dt):
#             continue
#         txt = strip_quotes_and_signatures(m["body"])
#         results.append({**m, "body": txt})
#     results.sort(key=lambda x: x["dt"])
#     return results

# def collect_sources_for_range(
#     start_dt: datetime,
#     end_dt: datetime,
#     subject_filter: str,
#     uploaded_pdfs,
#     emails_override: List[dict] | None = None
# ) -> List[dict]:
#     docs = []
#     emails = emails_override if emails_override is not None else search_emails_in_range(start_dt, end_dt, subject_filter)
#     for m in emails:
#         docs.append({"kind":"email","dt":m["dt"],"title":m["subject"],"text":m["body"]})
#     for f in (uploaded_pdfs or []):
#         content = f.read()
#         text = read_pdf_bytes_to_text(content)
#         dt_guess = guess_date_from_filename(f.name) or datetime.now()
#         docs.append({"kind":"pdf","dt":dt_guess,"title":f.name,"text":text})
#     docs.sort(key=lambda x: x["dt"])
#     return docs

# # =========================
# # UI
# # =========================
# st.sidebar.title("Menu")
# page = st.sidebar.radio("Cat√©gories", ["Weekly","Monthly"], index=0)
# st.sidebar.caption("üíæ JSON dans ./data/")

# st.title("üóûÔ∏è Hybrids Financial Debts Views ‚Äî Weekly / Monthly")

# # ---- WEEKLY ----
# if page=="Weekly":
#     st.subheader("Weekly")

#     chosen_day = st.date_input("Choisir un jour de la semaine", value=date.today(), format="YYYY-MM-DD")
#     monday, sunday = monday_sunday(chosen_day)
#     st.caption(f"Semaine: {monday.isoformat()} ‚Üí {sunday.isoformat()} (th√®mes puis chrono du plus ancien au plus r√©cent)")

#     # Browse e-mails (semaine)
#     start_dt = datetime.combine(monday, datetime.min.time())
#     end_dt   = datetime.combine(sunday, datetime.max.time())

#     colA, colB = st.columns([1,1])
#     with colA:
#         if st.button("üîç Parcourir les e-mails (semaine)"):
#             emails = search_emails_in_range(start_dt, end_dt, SUBJECT_WEEKLY_FILTER)
#             st.session_state["weekly_email_list"] = emails
#             st.session_state["weekly_selected_ids"] = {m["message_id"] for m in emails}
#     with colB:
#         st.caption("Filtre objet: " + SUBJECT_WEEKLY_FILTER)

#     if "weekly_email_list" in st.session_state:
#         with st.expander("üì¨ E-mails trouv√©s (cliquer pour voir / cocher √† inclure)", expanded=True):
#             sel = st.session_state.get("weekly_selected_ids", set())
#             new_sel = set(sel)
#             for m in st.session_state["weekly_email_list"]:
#                 mid = m["message_id"]
#                 checked = st.checkbox(
#                     f"[{m['dt'].strftime('%Y-%m-%d %H:%M')}] {m['subject']}",
#                     key=f"wk_mail_{mid}",
#                     value=(mid in sel)
#                 )
#                 if checked: new_sel.add(mid)
#                 else:       new_sel.discard(mid)
#                 with st.expander("Voir l‚Äôaper√ßu", expanded=False):
#                     st.write((m["body"] or "")[:1500] + ("..." if len(m["body"])>1500 else ""))
#             st.session_state["weekly_selected_ids"] = new_sel

#     # Choix affichage rubriques
#     st.markdown("**Rubriques √† afficher**")
#     cols = st.columns(5); chosen=[]
#     for i,f in enumerate(DAILY_FIELDS):
#         with cols[i%5]:
#             if st.checkbox(f.replace('_',' ').title(), value=True, key=f"w_{f}"): chosen.append(f)
#     chosen = chosen or DAILY_FIELDS

#     # PDFs optionnels (si pas d'emails, exiger ‚â•5)
#     st.markdown("**D√©posez des e-mails/notes au format PDF (optionnel si vous utilisez les e-mails)**")
#     pdf_files = st.file_uploader("Glisser-d√©poser ici", type=["pdf"], accept_multiple_files=True)

#     if st.button("‚öôÔ∏è G√©n√©rer le Weekly"):
#         selected_emails = []
#         if "weekly_email_list" in st.session_state:
#             ids = st.session_state.get("weekly_selected_ids", set())
#             selected_emails = [m for m in st.session_state["weekly_email_list"] if m["message_id"] in ids]

#         pdf_cnt = len(pdf_files or [])
#         if not selected_emails and pdf_cnt < 5:
#             st.error("S√©lectionnez au moins **un e-mail** OU d√©posez **‚â• 5 PDFs** pour g√©n√©rer le Weekly.")
#         else:
#             emails_override = selected_emails if selected_emails else []
#             docs = collect_sources_for_range(start_dt, end_dt, SUBJECT_WEEKLY_FILTER, pdf_files, emails_override=emails_override)
#             if not docs:
#                 st.warning("Aucune source s√©lectionn√©e/trouv√©e (v√©rifiez e-mails s√©lectionn√©s ou PDFs).")
#             else:
#                 # --- NOUVEAU : 1 seul appel de tri ---
#                 categorized = classify_all_at_once(docs)
#                 weekly_input = "\n\n".join(
#                     f"## {k}\n{categorized[k]}" for k in DAILY_FIELDS if categorized.get(k, "").strip()
#                 )
#                 # 1 appel de synth√®se
#                 llm = synth_llm_weekly(weekly_input)
#                 fields = {k: dedupe_bullets(llm.get(k,"")) for k in DAILY_FIELDS}
#                 global_summary = dedupe_bullets(llm.get("global_summary",""))
#                 week_key = iso_week_key(monday)

#                 payload = {
#                     "week": week_key,
#                     "window": [monday.isoformat(), sunday.isoformat()],
#                     "fields": fields,
#                     "global_summary": global_summary,
#                     "sources": [{"title":d["title"],"kind":d["kind"],"dt":d["dt"].isoformat()} for d in docs]
#                 }
#                 st.session_state["weekly_preview"] = payload
#                 st.success("Weekly g√©n√©r√©.")

#     if "weekly_preview" in st.session_state:
#         wk = st.session_state["weekly_preview"]
#         st.markdown(f"### Weekly : {wk['week']}")
#         st.write("Fen√™tre :", " ‚Üí ".join(wk.get("window", [])) or "N/A")
#         with st.expander("R√©sum√© global", expanded=True):
#             st.markdown(md_block(wk.get("global_summary","")))
#         for k in DAILY_FIELDS:
#             if k in chosen:
#                 with st.expander(k.replace("_"," ").title(), expanded=False):
#                     st.markdown(md_block(wk["fields"].get(k,"")))
#         with st.expander("Sources utilis√©es"):
#             for s in wk.get("sources", []):
#                 st.write(f"- [{s['kind']}] {s['dt']} ‚Äî {s['title']}")
#         if st.button("üíæ Enregistrer ce Weekly"):
#             save_json(weekly_path(wk['week']), wk); st.success(f"Weekly sauvegard√© : {weekly_path(wk['week'])}")

# # ---- MONTHLY ----
# elif page=="Monthly":
#     st.subheader("Monthly")

#     today = date.today()
#     y = st.number_input("Ann√©e", value=today.year, step=1)
#     m = st.number_input("Mois", 1, 12, value=today.month, step=1)
#     first = date(int(y), int(m), 1)
#     nxt   = first + relativedelta(months=1)
#     st.caption(f"P√©riode: {first.isoformat()} ‚Üí {(nxt - timedelta(days=1)).isoformat()} (th√®mes puis chrono du plus ancien au plus r√©cent)")

#     # Browse e-mails (mois)
#     start_dt = datetime.combine(first, datetime.min.time())
#     end_dt   = datetime.combine(nxt - timedelta(seconds=1), datetime.max.time())

#     colA, colB = st.columns([1,1])
#     with colA:
#         if st.button("üîç Parcourir les e-mails (mois)"):
#             emails = search_emails_in_range(start_dt, end_dt, SUBJECT_MONTHLY_FILTER)
#             st.session_state["monthly_email_list"] = emails
#             st.session_state["monthly_selected_ids"] = {m["message_id"] for m in emails}
#     with colB:
#         st.caption("Filtre objet: " + SUBJECT_MONTHLY_FILTER)

#     if "monthly_email_list" in st.session_state:
#         with st.expander("üì¨ E-mails trouv√©s (cliquer pour voir / cocher √† inclure)", expanded=True):
#             sel = st.session_state.get("monthly_selected_ids", set())
#             new_sel = set(sel)
#             for m in st.session_state["monthly_email_list"]:
#                 mid = m["message_id"]
#                 checked = st.checkbox(
#                     f"[{m['dt'].strftime('%Y-%m-%d %H:%M')}] {m['subject']}",
#                     key=f"mo_mail_{mid}",
#                     value=(mid in sel)
#                 )
#                 if checked: new_sel.add(mid)
#                 else:       new_sel.discard(mid)
#                 with st.expander("Voir l‚Äôaper√ßu", expanded=False):
#                     st.write((m["body"] or "")[:1500] + ("..." if len(m["body"])>1500 else ""))
#             st.session_state["monthly_selected_ids"] = new_sel

#     # Rubriques √† afficher
#     st.markdown("**Rubriques √† afficher**")
#     cols = st.columns(5); chosen=[]
#     for i,f in enumerate(DAILY_FIELDS):
#         with cols[i%5]:
#             if st.checkbox(f.replace('_',' ').title(), value=True, key=f"m_{f}"): chosen.append(f)
#     chosen = chosen or DAILY_FIELDS

#     # PDFs optionnels
#     st.markdown("**(Optionnel) D√©poser des PDF**")
#     pdf_files = st.file_uploader("Glisser-d√©poser des PDF (facultatif)", type=["pdf"], accept_multiple_files=True, key="monthly_pdf")

#     if st.button("‚öôÔ∏è G√©n√©rer le Monthly"):
#         emails_override = None
#         if "monthly_email_list" in st.session_state:
#             ids = st.session_state.get("monthly_selected_ids", set())
#             emails_override = [m for m in st.session_state["monthly_email_list"] if m["message_id"] in ids]

#         docs = collect_sources_for_range(start_dt, end_dt, SUBJECT_MONTHLY_FILTER, pdf_files, emails_override=emails_override)
#         if not docs:
#             st.warning("Aucune source s√©lectionn√©e/trouv√©e.")
#         else:
#             # 1 seul appel de tri
#             categorized = classify_all_at_once(docs)
#             monthly_input = "\n\n".join(
#                 f"## {k}\n{categorized[k]}" for k in DAILY_FIELDS if categorized.get(k, "").strip()
#             )
#             # 1 appel de synth√®se mensuelle
#             llm = synth_llm_monthly(monthly_input)
#             fields = {k: dedupe_bullets(llm.get(k,"")) for k in DAILY_FIELDS}
#             global_summary = dedupe_bullets(llm.get("global_summary",""))
#             month_key = f"{int(y):04d}-{int(m):02d}"
#             payload = {
#                 "month": month_key,
#                 "fields": fields,
#                 "global_summary": global_summary,
#                 "sources":[{"title":d["title"],"kind":d["kind"],"dt":d["dt"].isoformat()} for d in docs]
#             }
#             st.session_state["monthly_preview"]=payload
#             st.success("Monthly g√©n√©r√©.")

#     if "monthly_preview" in st.session_state:
#         mo = st.session_state["monthly_preview"]
#         st.markdown(f"### Monthly : {mo['month']}")
#         with st.expander("R√©sum√© global", expanded=True):
#             st.markdown(md_block(mo.get("global_summary","")))
#         for k in DAILY_FIELDS:
#             if k in chosen:
#                 with st.expander(k.replace("_"," ").title(), expanded=False):
#                     st.markdown(md_block(mo["fields"].get(k,"")))
#         with st.expander("Sources utilis√©es"):
#             for s in mo.get("sources", []):
#                 st.write(f"- [{s['kind']}] {s['dt']} ‚Äî {s['title']}")
#         if st.button("üíæ Enregistrer ce Monthly"):
#             save_json(monthly_path(mo['month']), mo); st.success(f"Monthly sauvegard√© : {monthly_path(mo['month'])}")


from __future__ import annotations
import os, re, json, imaplib, email, io
from email.header import decode_header, make_header
from email.utils import parsedate_to_datetime
from pathlib import Path
from datetime import date, datetime, timedelta
from typing import Dict, List, Tuple

import streamlit as st
from dateutil.relativedelta import relativedelta

# =========================
# GATE (facultatif)
# =========================
gate = st.secrets.get("APP_PASSWORD", "")
if gate:
    pw = st.text_input("Mot de passe", type="password")
    if pw != gate:
        st.stop()

# =========================
# CONFIG
# =========================
st.set_page_config(page_title="Weekly / Monthly ‚Äì Hybrids Views", layout="wide")

# ‚ö†Ô∏è Lire les secrets depuis st.secrets (ne pas hardcoder)
GMAIL_ADDRESS       = st.secrets.get("GMAIL_ADDRESS", "")
GMAIL_APP_PASSWORD  = st.secrets.get("GMAIL_APP_PASSWORD", "")
OPENAI_API_KEY      = st.secrets.get("OPENAI_API_KEY", "")
OPENAI_MODEL        = st.secrets.get("OPENAI_MODEL", "gpt-4o-mini")

SUBJECT_WEEKLY_FILTER  = st.secrets.get("SUBJECT_WEEKLY_FILTER",  "Hybrids Financial Debts Views")
SUBJECT_MONTHLY_FILTER = st.secrets.get("SUBJECT_MONTHLY_FILTER", "Hybrids Financial Debts Views Weekly")

GMAIL_SEARCH_DAYS = int(st.secrets.get("GMAIL_SEARCH_DAYS", 30))
MAX_EMAILS_FETCH  = int(st.secrets.get("MAX_EMAILS_FETCH", 400))

DATA_DIR    = Path("data")
WEEKLY_DIR  = DATA_DIR / "weekly"
MONTHLY_DIR = DATA_DIR / "monthly"
for d in (DATA_DIR, WEEKLY_DIR, MONTHLY_DIR): d.mkdir(parents=True, exist_ok=True)

# Rubriques (ordre th√©matique)
DAILY_FIELDS = ["ma_and_ratings","results","financial_credit_spreads","primary_market","other_important_infos"]

# =========================
# HELPERS FS/FORMAT
# =========================
def weekly_path(w: str) -> Path:   return WEEKLY_DIR / f"{w}.json"
def monthly_path(m: str) -> Path:  return MONTHLY_DIR / f"{m}.json"

def load_json(p: Path, default):
    if p.exists():
        try: return json.loads(p.read_text(encoding="utf-8"))
        except: return default
    return default

def save_json(p: Path, payload): p.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

def iso_week_key(d: date) -> str:
    y, w, _ = d.isocalendar()
    return f"{y}-W{w:02d}"

def monday_sunday(d: date) -> Tuple[date, date]:
    monday = d - timedelta(days=d.weekday())
    sunday = monday + timedelta(days=6)
    return monday, sunday

def md_block(text: str) -> str:
    return (text or "").strip().replace("\n", "  \n") or "_(vide)_"

def normalize_line(s: str) -> str:
    return re.sub(r"\s+", " ", s.strip().strip("‚Ä¢-‚Äì¬∑* ")).lower()

def dedupe_bullets(text: str) -> str:
    seen, out = set(), []
    for ln in (text or "").splitlines():
        base = normalize_line(ln)
        if base and base not in seen:
            seen.add(base)
            out.append(f"- {ln.strip().strip('‚Ä¢-‚Äì¬∑* ')}")
    return "\n".join(out)

def summarize_texts(texts: List[str], max_bullets=20) -> str:
    tokens = []
    for t in texts or []:
        for ln in str(t or "").splitlines():
            s = ln.strip(" ‚Ä¢-‚Äì\t ")
            if s: tokens.append(s)
    uniq = list(dict.fromkeys(tokens))[:max_bullets]
    return "\n".join(f"- {b}" for b in uniq)

# =========================
# GMAIL
# =========================
def _decode_header(s) -> str:
    try: return str(make_header(decode_header(s or "")))
    except: return s or ""

def _strip_html(html: str) -> str:
    t = re.sub(r"(?is)<(script|style).*?>.*?(</\1>)", " ", html)
    t = re.sub(r"(?s)<.*?>", " ", t)
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n\s*\n\s*\n+", "\n\n", t)
    return t.strip()

def _message_to_text(msg: email.message.Message) -> str:
    if msg.is_multipart():
        for part in msg.walk():
            if part.get_content_type() == "text/plain" and "attachment" not in str(part.get("Content-Disposition") or "").lower():
                try: return part.get_payload(decode=True).decode(part.get_content_charset() or "utf-8", errors="replace")
                except: pass
        for part in msg.walk():
            if part.get_content_type() == "text/html" and "attachment" not in str(part.get("Content-Disposition") or "").lower():
                try: return _strip_html(part.get_payload(decode=True).decode(part.get_content_charset() or "utf-8", errors="replace"))
                except: pass
        payloads=[]
        for part in msg.walk():
            if part.get_content_maintype()=="text":
                try: payloads.append(part.get_payload(decode=True).decode(part.get_content_charset() or "utf-8", errors="replace"))
                except: pass
        return "\n\n".join(payloads)
    else:
        try:
            payload = msg.get_payload(decode=True)
            if payload is None: return str(msg.get_payload())
            text = payload.decode(msg.get_content_charset() or "utf-8", errors="replace")
            return _strip_html(text) if msg.get_content_type()=="text/html" else text
        except: return str(msg.get_payload())

def fetch_gmail_messages(newer_than_days=30, limit=400) -> List[dict]:
    res=[]
    if not GMAIL_ADDRESS or not GMAIL_APP_PASSWORD:
        return res
    try:
        imap = imaplib.IMAP4_SSL("imap.gmail.com", 993)
        imap.login(GMAIL_ADDRESS, GMAIL_APP_PASSWORD)
        ok,_ = imap.select("INBOX")
        if ok!="OK": raise RuntimeError("INBOX non s√©lectionnable")
        since = (datetime.utcnow() - timedelta(days=newer_than_days)).strftime("%d-%b-%Y")
        ok, data = imap.search(None, f'(SINCE {since})')
        if ok!="OK": return res
        ids = (data[0].split() or [])[-limit:]
        for msg_id in reversed(ids):
            ok, msg_data = imap.fetch(msg_id, "(RFC822)")
            if ok!="OK": continue
            msg  = email.message_from_bytes(msg_data[0][1])
            subj = _decode_header(msg.get("Subject",""))
            body = _message_to_text(msg)
            try:
                dt = parsedate_to_datetime(msg.get("Date"))
                if dt and dt.tzinfo: dt = dt.astimezone().replace(tzinfo=None)
            except:
                dt = datetime.now()
            mid = (msg.get("Message-Id") or msg.get("Message-ID") or "").strip() or f"INBOX:{msg_id.decode()}"
            res.append({"subject": subj, "body": body, "dt": dt, "message_id": mid})
        imap.logout()
    except Exception as e:
        st.error(f"Erreur Gmail IMAP: {e}")
    return res

# =========================
# CLEAN BODY (quotes/signatures)
# =========================
QUOTE_PATTERNS = [
    r"^On .+ wrote:$", r"^Le .+ a √©crit :", r"^From: .+$", r"^-{2,}\s*Original Message\s*-{2,}$",
    r"^> .*", r"^__+$", r"^\s*De : .+$", r"^\s*Envoy√© : .+$"
]
def strip_quotes_and_signatures(txt: str) -> str:
    lines, out = (txt or "").splitlines(), []
    for ln in lines:
        if any(re.match(p, ln.strip(), re.IGNORECASE) for p in QUOTE_PATTERNS): break
        out.append(ln)
    joined = "\n".join(out)
    joined = re.sub(r"(?is)this message.*?confidential.*", "", joined)
    joined = re.sub(r"(?is)ce message.*?confidentiel.*", "", joined)
    return joined.strip()

# =========================
# PDF LOADER
# =========================
def read_pdf_bytes_to_text(data: bytes) -> str:
    try:
        import PyPDF2
        reader = PyPDF2.PdfReader(io.BytesIO(data))
        pages = []
        for p in reader.pages:
            try:
                pages.append(p.extract_text() or "")
            except:
                pages.append("")
        return "\n".join(pages)
    except Exception as e:
        st.error(f"Lecture PDF: {e}")
        return ""

def guess_date_from_filename(name: str) -> datetime|None:
    m = re.search(r"(20\d{2})[-_\.]?(0[1-9]|1[0-2])[-_\.]?(0[1-9]|[12]\d|3[01])", name)
    if not m: return None
    try:
        s = f"{m.group(1)}-{m.group(2)}-{m.group(3)}"
        return datetime.fromisoformat(s)
    except:
        return None

# =========================
# OPENAI (Classifier + Synth√®ses)
# =========================
from openai import OpenAI
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

DAILY_SYS = (
    "Tu es un assistant financier. Classe TOUT le contenu pertinent dans EXACTEMENT ces 5 cat√©gories.\n"
    "Aucune paraphrase. Recopie le texte source tel quel, en phrases compl√®tes (jamais tronqu√©es).\n"
    "1 item = 1 phrase autonome (pas de virgule finale/parenth√®se ouverte).\n"
    "Si des retours √† la ligne coupent une phrase, RECOLLE les segments.\n"
    "IMPORTANT : conserve l‚ÄôORDRE CHRONOLOGIQUE d‚Äôapparition du texte source.\n"
    "IMPORTANT : pr√©fixe chaque item par la date au format [YYYY-MM-DD] si elle est fournie.\n"
    "Exclure signatures/politesse/disclaimers.\n"
    "En fran√ßais."
)
DAILY_USER_TMPL = """Texte :
---
[[EMAIL_BODY]]
---
Retourne un JSON strict:
{
  "financial_credit_spreads": ["..."],
  "primary_market": ["..."],
  "results": ["..."],
  "ma_and_ratings": ["..."],
  "other_important_infos": ["..."]
}"""

WEEKLY_SYS = (
    "Tu es analyste cr√©dit. √Ä partir d‚Äôun contenu agr√©g√© (d√©j√† class√©), g√©n√®re un r√©sum√© hebdomadaire PROPRE.\n"
    "Objectifs: d√©doublonner, regrouper ce qui se r√©p√®te, garder chiffres/√©ch√©ances exacts, z√©ro hallucination, phrases compl√®tes.\n"
    "Style: fluide, coh√©rent, structur√©. Ne pas tronquer.\n"
)
WEEKLY_USER_TMPL = """Contenu agr√©g√© (par rubrique) :
---
[[WEEKLY_INPUT]]
---
Exemples de sortie attendue 'global_summary' :
{
  "global_summary": " 
  
  Sur le Cr√©dit Financier, les spreads affichent des √©volutions contrast√©es : +1 pdb pour les Seniors bancaires, -1 pdb pour les T2 bancaires, +2 pdb pour les AT1 (-2 pdb pour les ‚Ç¨AT1) et, du c√¥t√© des assureurs, les Seniors √©taient √† +2 pdb et les Subordonn√©s √† -2 pdb. Globalement, le march√© est rest√© assez stable. Les performances se sont r√©v√©l√©es l√©g√®rement positives dans l‚Äôensemble : les Seniors, tant bancaires qu‚Äôassureurs, ont progress√© de +0,1 %, les T2 de +0,2 %, les AT1 de +0,3 % et les Subordonn√©s assurance de +0,4 %.

Le march√© primaire demeure assez calme cette semaine, avec seulement quelques √©missions. Du c√¥t√© des subordonn√©es, Generali a √©mis un Tier 2 bullet 11Y (√† un faible spread de MS+155pdb), Malakoff Humanis un Tier 2 bullet 10Y (MS+210pdb), BNP a √©mis ‚Ç¨1md de Tier 2 10,5NC5,5 ans √† MS+155pdb, Gothaer a √©mis un Tier 2 20NC10 ans de ‚Ç¨300m √† MS+250 pdb, et FMO a √©mis un Tier 2 11NC6 ans de ‚Ç¨300m √† seulement 75 pdb au-dessus du swap. C√¥t√© senior, Julius Baer a √©mis ‚Ç¨500m √† 5Y et LHV, bas√© en Estonie, a √©mis une petite tranche de ‚Ç¨60m‚Ç¨ 4NC3.

La semaine a √©t√© anim√©e par de nombreux titres li√©s aux fusions-acquisitions. Premi√®rement, apr√®s plusieurs jours de ¬´ rumeurs ¬ª sur le march√©, BPCE a officiellement annonc√© avoir sign√© un protocole d‚Äôaccord pour acqu√©rir NovoBanco, en rachetant 75 % du capital d√©tenu par le fonds de private equity Lone Star Funds. √Ä l‚Äôissue de la transaction, le ratio CET1 du Groupe BPCE resterait sup√©rieur √† 15 % (contre 16,2 % √† fin 2024). La banque est √©galement entr√©e en n√©gociation avec le gouvernement portugais pour l‚Äôacquisition des 25 % restants, √† la m√™me valorisation que celle de Lone Star.

Deuxi√®mement, le PDG d‚ÄôUniCredit, Andrea Orcel, a indiqu√© qu‚Äôune offre de rachat sur Commerzbank √©tait peu probable aux valorisations actuelles, en raison de la r√©cente hausse du titre. Orcel a sugg√©r√© qu‚Äôil pourrait attendre jusqu‚Äôen 2027 avant d‚Äôenvisager une acquisition totale. Il pr√©voit d‚Äôobtenir d‚Äôici la fin du mois les autorisations r√©glementaires pour convertir des d√©riv√©s en actions, ce qui pourrait porter la participation d‚ÄôUniCredit √† pr√®s de 30 %. Orcel a laiss√© entendre que cette participation pourrait conf√©rer des ¬´ droits et une influence ¬ª, tout en soulignant que l‚Äôincertitude prolong√©e n‚Äô√©tait b√©n√©fique pour aucune des deux parties.

Troisi√®mement, concernant l‚Äôoffre en cours sur Banco BPM, Orcel a d√©clar√© qu‚Äôil pourrait se retirer si le gouvernement italien ne clarifiait pas les conditions impos√©es √† la transaction. Il estime actuellement √† 20 % ou moins la probabilit√© de poursuivre l‚Äôop√©ration.

Enfin, sur le plan des notations, S&P a relev√© la note de Sava Re et de Zavarovalnica Sava, deux assureurs slov√®nes, de A √† A+ avec une perspective stable, refl√©tant leur solide r√©silience financi√®re et leur position sur le march√©. Cette am√©lioration s‚Äôest √©galement traduite par une hausse de la notation des obligations Tier 2 de Sava Re, passant de BBB+ √† A-. Cette d√©cision fait suite √† l‚Äôam√©lioration de la note souveraine de la Slov√©nie, pass√©e la semaine derni√®re de AA- √† AA, ce qui a fortement influenc√© la r√©√©valuation de la position de Sava Re.
  ", or "global_summary": " Sur les dettes subordonn√©es financi√®res cette semaine, les spreads se sont resserr√©s sur tous les rangs de la structure de capital : -5bps seniors, -10bps T2, -21bps AT1 (-25bps ‚Ç¨AT1), -9bps subordonn√©es d‚Äôassurance. La performance a √©t√© positive sur les segments subordonn√©s : -0.1% seniors bancaires et -0.2% seniors d‚Äôassurance, +0.1% T2 et +0.2% subordonn√©es d‚Äôassurance, +0.6% AT1 (+0.7% ‚Ç¨AT1).

 Nous avons eu quelques √©missions de Subordonn√©es cette semaine. En Tier 2, Hiscox a √©mis un 11NC10 pour $500m (coupon 7%, reset 302pdb), en parall√®le d'une offre de rachat sur un Tier 2 existant libell√© en livres sterling. En AT1, Nationwide a √©mis un PerpNC6 pour ¬£700m (coupon 7,875%, reset de 359pdb), en parall√®le d'une offre de rachat sur deux AT1 existants de Virgin Money. Enfin, Attica, la cinqui√®me plus grande banque grecque, a √©mis un AT1 (‚Ç¨100m, PerpNC5,5, coupon 9,375%, reset de 727pdb) et un Tier 2 (‚Ç¨150m, 10 ans, coupon 7,375%, reset de 510pdb).

 S&P a relev√© la not√© de Crelan de A- √† A avec une perspective stable.

 Au sujet des fusions & acquisitions, en Pologne, deux des plus grandes institutions financi√®res polonaises (la compagnie d'assurance PZU et la banque Pekao) ont annonc√© la signature d'un protocole d'accord concernant la r√©organisation du groupe PZU.

 Autrement, le gouvernement suisse a publi√© ses propositions de r√©forme bancaire, qui concerneront principalement UBS. La proposition inclut une augmentation du capital d√©tenu localement contre les filiales √©trang√®res, passant de 60 % √† 100 %, ce qui, √† lui seul, pourrait n√©cessiter 23 milliards de dollars suppl√©mentaires de capital pour la principale entit√© bas√©e en Suisse. Les r√©formes seront mises en place progressivement sur une p√©riode de 6 √† 8 ans. Le gouvernement a √©galement sugg√©r√© une r√©duction de l‚Äôutilisation des AT1 comme capital d‚Äôenviron 8 milliards de dollars, laissant un besoin suppl√©mentaire de 18 milliards de dollars en capital "allou√© √† la continuit√© d‚Äôexploitation". Le gouvernement est en train de finaliser le projet de loi, qui sera soumis au Parlement, la nouvelle l√©gislation pouvant entrer en vigueur d√®s 2028 au plus t√¥t.

 En zone euro, les volumes de pr√™ts ont augment√© de +1,8% sur un an en avril (contre +1,6% sur un an en mars), avec une nouvelle production solide √† +11% sur un an, et les nouvelles souscriptions de pr√™ts immobilier en hausse de +40% sur un an (et de mani√®re similaire depuis d√©but 2025). La r√©mun√©ration des d√©p√¥ts √† terme continue de baisser, en ligne avec les taux de la BCE. Ces indicateurs sont favorables au revenu net d'int√©r√™ts des banques, qui sera soutenu par une bonne croissance des volumes et une baisse du co√ªt de financement"

-----
Attendu (JSON strict) :
{
  "global_summary": "Entre 300 et 500 mots. Bien s'inspirer des exemples donn√©s ci dessous pour la structure. paragraphe structur√©, fluide. Un retour √† la ligne par th√®me (m&a, primary market, results, spreads, autres infos).",
  "ma_and_ratings": ["2 √† 8 puces propres"],
  "results": ["2 √† 8 puces propres"],
  "financial_credit_spreads": ["2 √† 8 puces propres"],
  "primary_market": ["2 √† 8 puces propres"],
  "other_important_infos": ["2 √† 8 puces propres"]
}"""

MONTHLY_SYS = (
    "Tu es analyste cr√©dit. √Ä partir de weeklies, produis une synth√®se mensuelle.\n"
    "Objectifs: tendances du mois, th√®mes dominants, √©v√©nements majeurs, risques et pipeline/primaire.\n"
    "Conserve chiffres/√©ch√©ances exacts, z√©ro hallucination; phrases compl√®tes.\n"
    "Style: fluide, coh√©rent, structur√©. Ne pas tronquer. Pas de redondance.\n"
)
MONTHLY_USER_TMPL = """Contenu agr√©g√© weeklies (par rubrique) :
---
[[MONTHLY_INPUT]]
---

Exemples de sortie attendue 'global_summary' :
{
  "global_summary": " Mai 2025 :
Le mois de mai a √©t√© positif pour les dettes financi√®res, qui poursuivent leur redressement et effacent m√™me compl√®tement le sell-off des droits douaniers en termes de spread.
 
Les spreads bancaires se sont donc resserr√©s sur le mois : -14 bps sur les Senior, -20 bps sur les Tier 2, et -41 bps sur les AT1 (-30 bps AT1 euros). Pour les assureurs, nous avons eu un resserrement de -11 bps sur les Senior et de -21 bps sur les Subordonn√©es.
Sur les taux, la courbe ‚Ç¨ est globalement stable sur le mois, tandis que les courbes ¬£ et $ se sont tendues parall√®lement d‚Äôenviron 20 bps.
Cela s‚Äôest traduit par une performance positive sur l‚Äôensemble de la structure de capital, avec une progression de +0,5 % pour les Seniors, +0,7 % pour les Tier 2, +1,6 % pour les AT1 (+1,7 % pour les AT1 en euros), et respectivement +0,4 % et +1,1 % pour les Seniors et Subordonn√©es d‚Äôassurance.
Dans ces conditions, le march√© primaire a √©t√© particuli√®rement actif post-publication des r√©sultats, et bien accueilli avec des carnets d‚Äôordres solides.
 
Les r√©sultats du premier trimestre 2025 des institutions financi√®res europ√©ennes ont continu√© d‚Äô√™tre excellents, c√¥t√© banques comme assurances, avec des RoTE entre 10 % et 16 % en moyenne, avec √† ce stade des guidances qui ne sont pas affect√©es par les droits de douane.
 
En mati√®re de fusions-acquisitions, Santander a rejet√© l‚Äôoffre de NatWest pour sa banque de d√©tail britannique, tandis qu'Erste Bank a acquis une participation de 49 % dans Santander Bank Polska. Au Royaume-Uni, Chesnara envisage le rachat de l‚Äôactivit√© assurance vie de HSBC UK, et Cr√©dit Agricole a pris une participation de 9,9 % dans la banque belge Crelan. En Islande, Kvika banki a re√ßu des indications d‚Äôint√©r√™t de Arion Banki et Islandbanki, et UniCredit a augment√© sa participation dans Alpha Bank √† ~20 %.
 
Les agences de notation ont continu√© √† proc√©der √† des rel√®vements de notes : Moody‚Äôs a relev√© la note de Bank of Cyprus √† A3, BCP √† A2, Novo Banco √† Baa1, Monte dei Paschi √† Baa1. Fitch a relev√© la note de Bank of Ireland √† A- et Permanent TSB √† BBB, Bank Millennium √† BBB-, tout en pla√ßant sous surveillance √©volutive la note de Mediobanca en raison des incertitudes li√©es au M&A (rachat par Monte ou bien rachat de Banca Generali).
 OR 
Avril 2025 :
Les performances d‚Äôavril, stables √† l√©g√®rement positives, masquent un mois volatil qui a mal commenc√©, principalement impact√© par l‚Äôannonce des ¬´ tarifs r√©ciproques ¬ª le 2 avril. Cela a cr√©√© un environnement de march√© tr√®s n√©gatif jusqu‚Äôau 9 avril, date √† laquelle une pause de 90 jours a √©t√© annonc√©e. Les march√©s se sont ensuite redress√©s et ont termin√© le mois soit stables, soit positifs gr√¢ce au resserrement des spreads et √† la baisse g√©n√©rale des taux.
 
Les spreads ont r√©cup√©r√© 60% de leur √©cartement du d√©but du mois d‚Äôavril, mais ont tout de m√™me fini plus √©cart√©s dans tous les segments de la structure du capital : les Seniors se sont √©cart√©s de +10pdb, les Tier 2 IG de +20pdb et les AT1 de +45pdb.
 
En revanche, les taux ont baiss√© dans toutes les devises G3 : environ -30pdb en ‚Ç¨, entre -28 et +11pdb en $ et entre -40 et -23pdb en ¬£. Ainsi, les performances sont positives dans tous les segments sauf les AT1, avec une performance de +1,0%/+1,1% pour les dettes Seniors des banques et assureurs, +0,8% pour les Tier 2 IG et +0,7% pour les dettes Subordonn√©es des assureurs, tandis que les AT1 ont termin√© stables (AT1 en euros) ou l√©g√®rement n√©gatifs √† -0,3 % (AT1 toutes devises).
 
Le march√© primaire a √©t√© tr√®s calme apr√®s la chute des march√©s au d√©but du mois, suivie des p√©riodes de blackout li√©es √† la publication des r√©sultats du 1Q25. Quoi qu‚Äôil en soit, les rares √©missions r√©alis√©es ont √©t√© bien accueillies et √©mises avec peu ou pas de prime de nouvelle √©mission (NIP), ce qui montre la disponibilit√© de liquidit√©s pr√™tes √† √™tre investies.
 
La BCE a une nouvelle fois abaiss√© ses taux √† 2,25 %, tout en adoptant un ton accommodant, mais en reconnaissant √©galement qu‚Äôelle avait d√©sormais atteint la neutralit√©.
 
En ce qui concerne les r√©sultats, les entreprises europ√©ennes du secteur financier et des assurances (FIG) ayant pr√©sent√© leurs r√©sultats pour le 1Q25 ont affich√© de tr√®s bonnes performances : BNPP a enregistr√© un profit net de 2,95 milliards d‚Äôeuros (-5 % en glissement annuel), Bankinter 270 millions (+35 %), Mapfre 276 millions (+28 %), Nordea 1,2 milliard (-9 %).
 
Les fusions-acquisitions (M&A) restent d'actualit√© dans le secteur FIG europ√©en, malgr√© la crise, avec plusieurs annonces de fusion durant le mois. Parmi celles-ci, Mediobanca a lanc√© une offre publique d‚Äôachat pour acqu√©rir Banca Generali, enti√®rement financ√©e par la vente de sa participation dans Generali. Baloise et Helvetia vont cr√©er le deuxi√®me plus grand assureur suisse avec une part de march√© domestique de 20 %. Ageas a acquis esure au Royaume-Uni pour 1,3 milliard de livres sterling, tandis que Banco BPM a r√©ussi √† obtenir 90 % du capital d‚ÄôAnima. Santander envisagerait de vendre sa filiale polonaise. Enfin, BBVA a re√ßu l‚Äôapprobation de la CNMC espagnole pour acheter Sabadell.
 
Les agences de notation ont continu√© √† proc√©der √† des rel√®vements de notes : Fitch et Moody‚Äôs ont am√©lior√© la note de mBank, passant de BBB- √† BBB et de Baa3 √† Ba1 respectivement. Bank Millennium a √©t√© relev√©e √† Ba1 contre Ba2, avec une perspective positive. OTP Bank a √©galement √©t√© rehauss√©e √† BBB contre BBB- par S&P. La Gr√®ce a √©t√© rehauss√©e √† BBB par S&P avec une perspective stable, tandis que les banques grecques ont vu leur note relev√©e d‚Äôun cran par Fitch : NBG et Eurobank √† BBB-, et Piraeus et Alpha Bank √† BB+. Swedbank a √©t√© relev√©e de Aa3 √† Aa2."
  



Attendu (JSON strict) :
{
  "global_summary": "Entre 300 et 600 mots. Inspir√©s des exemples ci-dessus. Recap des comptes rendus weekly, organis√©s par les th√©matiques ma, results, financials_credit_spread, primary and other important infos. FLuide, organis√©.).",
  "ma_and_ratings": ["2‚Äì10 puces propres"],
  "results": ["2‚Äì10 puces propres"],
  "financial_credit_spreads": ["2‚Äì10 puces propres"],
  "primary_market": ["2‚Äì10 puces propres"],
  "other_important_infos": ["2‚Äì10 puces propres"]
}"""

def _flatten_spaces(s: str) -> str:
    return re.sub(r"\s*\n+\s*", " ", str(s or "")).strip()

def _to_items(value) -> list[str]:
    if isinstance(value, list):
        return [_flatten_spaces(x).lstrip("‚Ä¢-‚Äì‚Äî* ").strip() for x in value if str(x).strip()]
    lines = [ln.rstrip() for ln in str(value or "").splitlines() if ln.strip()]
    items, cur = [], []
    for ln in lines:
        if re.match(r"^\s*[-‚Ä¢‚Äì‚Äî]\s+", ln):
            if cur: items.append(_flatten_spaces(" ".join(cur)))
            cur = [ln.lstrip().lstrip("-‚Ä¢‚Äì‚Äî* ").strip()]
        else:
            cur.append(ln.strip())
    if cur: items.append(_flatten_spaces(" ".join(cur)))
    items = [re.sub(r"\s+", " ", it).strip() for it in items if it and it.strip()]
    return items

def classify_with_openai(text: str) -> Dict[str, str]:
    if not client:
        return {k:"" for k in DAILY_FIELDS}
    body = (text or "")[:80000]
    usr  = DAILY_USER_TMPL.replace("[[EMAIL_BODY]]", body)
    try:
        r = client.chat.completions.create(
            model=OPENAI_MODEL,
            response_format={"type": "json_object"},
            temperature=0.1,
            messages=[{"role":"system","content":DAILY_SYS},{"role":"user","content":usr}],
        )
        parsed = json.loads(r.choices[0].message.content or "{}")
    except Exception as e:
        st.error(f"OpenAI error (classify): {e}")
        parsed = {}
    cleaned = {}
    for k in DAILY_FIELDS:
        raw = parsed.get(k, "")
        items = _to_items(raw)
        cleaned[k] = "\n".join(items)
    return cleaned

def synth_llm_weekly(weekly_input_text: str) -> Dict[str,str]:
    if not client:
        return {"global_summary": summarize_texts([weekly_input_text], 20), **{k: "" for k in DAILY_FIELDS}}
    usr = WEEKLY_USER_TMPL.replace("[[WEEKLY_INPUT]]", weekly_input_text[:80000])
    try:
        r = client.chat.completions.create(
            model=OPENAI_MODEL,
            response_format={"type":"json_object"},
            temperature=0.2,
            messages=[{"role":"system","content":WEEKLY_SYS},{"role":"user","content":usr}],
        )
        parsed = json.loads(r.choices[0].message.content or "{}")
    except Exception as e:
        st.error(f"OpenAI error (weekly): {e}")
        parsed = {}
    out = {"global_summary": _flatten_spaces(parsed.get("global_summary",""))}
    for k in DAILY_FIELDS:
        out[k] = "\n".join(_to_items(parsed.get(k,"")))
    return out

def synth_llm_monthly(monthly_input_text: str) -> Dict[str,str]:
    if not client:
        return {"global_summary": summarize_texts([monthly_input_text], 16), **{k: "" for k in DAILY_FIELDS}}
    usr = MONTHLY_USER_TMPL.replace("[[MONTHLY_INPUT]]", monthly_input_text[:120000])
    try:
        r = client.chat.completions.create(
            model=OPENAI_MODEL,
            response_format={"type":"json_object"},
            temperature=0.2,
            messages=[{"role":"system","content":MONTHLY_SYS},{"role":"user","content":usr}],
        )
        parsed = json.loads(r.choices[0].message.content or "{}")
    except Exception as e:
        st.error(f"OpenAI error (monthly): {e}")
        parsed = {}
    out = {"global_summary": _flatten_spaces(parsed.get("global_summary",""))}
    for k in DAILY_FIELDS:
        out[k] = "\n".join(_to_items(parsed.get(k,"")))
    return out

# =========================
# ONE-SHOT CLASSIFY HELPERS
# =========================
DATE_TAG_RE = re.compile(r"^\s*\[([0-9]{4}-[0-9]{2}-[0-9]{2})\]\s*(.*)")

def build_aggregate_text(docs: List[dict]) -> str:
    chunks = []
    for d in sorted(docs, key=lambda x: x["dt"]):
        ddate = d["dt"].date().isoformat()
        title = (d.get("title") or "").strip()
        text  = (d.get("text")  or "").strip()
        if not text:
            continue
        head = f"[{ddate}] {title}".strip()
        chunks.append(f"{head}\n{text}")
    return "\n\n---\n\n".join(chunks)

def ensure_dash_bullets(s: str) -> str:
    return "\n".join(f"- {ln.strip().lstrip('- ')}" for ln in (s or "").splitlines() if ln.strip())

def sort_items_by_date_prefix(text_block: str) -> str:
    rows = [ln.strip().lstrip("- ").strip() for ln in (text_block or "").splitlines() if ln.strip()]
    tagged = []
    for i, row in enumerate(rows):
        m = DATE_TAG_RE.match(row)
        if m:
            try:
                dt = datetime.fromisoformat(m.group(1)).date()
            except:
                dt = date.min
            txt = m.group(2).strip()
            tagged.append((dt, i, txt))
        else:
            tagged.append((date.min, i, row))
    tagged.sort(key=lambda t: (t[0], t[1]))
    return "\n".join(f"- {t[2]}" for t in tagged)

def classify_all_at_once(docs: List[dict]) -> Dict[str, str]:
    big_text = build_aggregate_text(docs)
    buckets  = classify_with_openai(big_text)  # UN seul appel
    out = {}
    for k in DAILY_FIELDS:
        value = ensure_dash_bullets(buckets.get(k, ""))
        value = dedupe_bullets(value)
        value = sort_items_by_date_prefix(value)
        out[k] = value
    return out

# =========================
# AGR√âGATION SOURCES
# =========================
def search_emails_in_range(start_dt: datetime, end_dt: datetime, subject_filter: str) -> List[dict]:
    results = []
    for m in fetch_gmail_messages(newer_than_days=GMAIL_SEARCH_DAYS, limit=MAX_EMAILS_FETCH):
        if not m.get("dt"): 
            continue
        if subject_filter.lower() not in (m["subject"] or "").lower():
            continue
        if not (start_dt <= m["dt"] <= end_dt):
            continue
        txt = strip_quotes_and_signatures(m["body"])
        results.append({**m, "body": txt})
    results.sort(key=lambda x: x["dt"])
    return results

def collect_sources_for_range(
    start_dt: datetime,
    end_dt: datetime,
    subject_filter: str,
    uploaded_pdfs,
    emails_override: List[dict] | None = None
) -> List[dict]:
    docs = []
    emails = emails_override if emails_override is not None else search_emails_in_range(start_dt, end_dt, subject_filter)
    for m in emails:
        docs.append({"kind":"email","dt":m["dt"],"title":m["subject"],"text":m["body"]})
    for f in (uploaded_pdfs or []):
        content = f.read()
        text = read_pdf_bytes_to_text(content)
        dt_guess = guess_date_from_filename(f.name) or datetime.now()
        docs.append({"kind":"pdf","dt":dt_guess,"title":f.name,"text":text})
    docs.sort(key=lambda x: x["dt"])
    return docs

# =========================
# UI
# =========================
st.sidebar.title("Menu")
page = st.sidebar.radio("Cat√©gories", ["Weekly","Monthly"], index=0)
st.sidebar.caption("üíæ JSON dans ./data/")

st.title("üóûÔ∏è Hybrids Financial Debts Views ‚Äî Weekly / Monthly")

# ---- WEEKLY ----
if page=="Weekly":
    st.subheader("Weekly")

    chosen_day = st.date_input("Choisir un jour de la semaine", value=date.today(), format="YYYY-MM-DD")
    monday, sunday = monday_sunday(chosen_day)
    st.caption(f"Semaine: {monday.isoformat()} ‚Üí {sunday.isoformat()} (th√®mes puis chrono du plus ancien au plus r√©cent)")

    # Browse e-mails (semaine)
    start_dt = datetime.combine(monday, datetime.min.time())
    end_dt   = datetime.combine(sunday, datetime.max.time())

    colA, colB = st.columns([1,1])
    with colA:
        if st.button("üîç Parcourir les e-mails (semaine)"):
            emails = search_emails_in_range(start_dt, end_dt, SUBJECT_WEEKLY_FILTER)
            st.session_state["weekly_email_list"] = emails
            st.session_state["weekly_selected_ids"] = {m["message_id"] for m in emails}
    with colB:
        st.caption("Filtre objet: " + SUBJECT_WEEKLY_FILTER)

    if "weekly_email_list" in st.session_state:
        with st.expander("üì¨ E-mails trouv√©s (cliquer pour voir / cocher √† inclure)", expanded=True):
            sel = st.session_state.get("weekly_selected_ids", set())
            new_sel = set(sel)
            for m in st.session_state["weekly_email_list"]:
                mid = m["message_id"]
                checked = st.checkbox(
                    f"[{m['dt'].strftime('%Y-%m-%d %H:%M')}] {m['subject']}",
                    key=f"wk_mail_{mid}",
                    value=(mid in sel)
                )
                if checked: new_sel.add(mid)
                else:       new_sel.discard(mid)
                with st.expander("Voir l‚Äôaper√ßu", expanded=False):
                    st.write((m["body"] or "")[:7000] + ("..." if len(m["body"])>7000 else ""))
            st.session_state["weekly_selected_ids"] = new_sel

    # Choix affichage rubriques
    st.markdown("**Rubriques √† afficher**")
    cols = st.columns(5); chosen=[]
    for i,f in enumerate(DAILY_FIELDS):
        with cols[i%5]:
            if st.checkbox(f.replace('_',' ').title(), value=True, key=f"w_{f}"): chosen.append(f)
    chosen = chosen or DAILY_FIELDS

    # PDFs optionnels (si pas d'emails, exiger ‚â•5)
    st.markdown("**D√©posez des e-mails/notes au format PDF (optionnel si vous utilisez les e-mails)**")
    pdf_files = st.file_uploader("Glisser-d√©poser ici", type=["pdf"], accept_multiple_files=True)

    if st.button("‚öôÔ∏è G√©n√©rer le Weekly"):
        selected_emails = []
        if "weekly_email_list" in st.session_state:
            ids = st.session_state.get("weekly_selected_ids", set())
            selected_emails = [m for m in st.session_state["weekly_email_list"] if m["message_id"] in ids]

        pdf_cnt = len(pdf_files or [])
        if not selected_emails and pdf_cnt < 5:
            st.error("S√©lectionnez au moins **un e-mail** OU d√©posez **‚â• 5 PDFs** pour g√©n√©rer le Weekly.")
        else:
            emails_override = selected_emails if selected_emails else []
            docs = collect_sources_for_range(start_dt, end_dt, SUBJECT_WEEKLY_FILTER, pdf_files, emails_override=emails_override)
            if not docs:
                st.warning("Aucune source s√©lectionn√©e/trouv√©e (v√©rifiez e-mails s√©lectionn√©s ou PDFs).")
            else:
                # --- NOUVEAU : 1 seul appel de tri ---
                categorized = classify_all_at_once(docs)
                weekly_input = "\n\n".join(
                    f"## {k}\n{categorized[k]}" for k in DAILY_FIELDS if categorized.get(k, "").strip()
                )
                # 1 appel de synth√®se
                llm = synth_llm_weekly(weekly_input)
                fields = {k: dedupe_bullets(llm.get(k,"")) for k in DAILY_FIELDS}
                global_summary = dedupe_bullets(llm.get("global_summary",""))
                week_key = iso_week_key(monday)

                payload = {
                    "week": week_key,
                    "window": [monday.isoformat(), sunday.isoformat()],
                    "fields": fields,
                    "global_summary": global_summary,
                    "sources": [{"title":d["title"],"kind":d["kind"],"dt":d["dt"].isoformat()} for d in docs]
                }
                st.session_state["weekly_preview"] = payload
                st.success("Weekly g√©n√©r√©.")

    if "weekly_preview" in st.session_state:
        wk = st.session_state["weekly_preview"]
        st.markdown(f"### Weekly : {wk['week']}")
        st.write("Fen√™tre :", " ‚Üí ".join(wk.get("window", [])) or "N/A")
        with st.expander("R√©sum√© global", expanded=True):
            st.markdown(md_block(wk.get("global_summary","")))
        for k in DAILY_FIELDS:
            if k in chosen:
                with st.expander(k.replace("_"," ").title(), expanded=False):
                    st.markdown(md_block(wk["fields"].get(k,"")))
        with st.expander("Sources utilis√©es"):
            for s in wk.get("sources", []):
                st.write(f"- [{s['kind']}] {s['dt']} ‚Äî {s['title']}")
        if st.button("üíæ Enregistrer ce Weekly"):
            save_json(weekly_path(wk['week']), wk); st.success(f"Weekly sauvegard√© : {weekly_path(wk['week'])}")

# ---- MONTHLY ----
elif page=="Monthly":
    st.subheader("Monthly")

    today = date.today()
    y = st.number_input("Ann√©e", value=today.year, step=1)
    m = st.number_input("Mois", 1, 12, value=today.month, step=1)
    first = date(int(y), int(m), 1)
    nxt   = first + relativedelta(months=1)
    st.caption(f"P√©riode: {first.isoformat()} ‚Üí {(nxt - timedelta(days=1)).isoformat()} (th√®mes puis chrono du plus ancien au plus r√©cent)")

    # Browse e-mails (mois)
    start_dt = datetime.combine(first, datetime.min.time())
    end_dt   = datetime.combine(nxt - timedelta(seconds=1), datetime.max.time())

    colA, colB = st.columns([1,1])
    with colA:
        if st.button("üîç Parcourir les e-mails (mois)"):
            emails = search_emails_in_range(start_dt, end_dt, SUBJECT_MONTHLY_FILTER)
            st.session_state["monthly_email_list"] = emails
            st.session_state["monthly_selected_ids"] = {m["message_id"] for m in emails}
    with colB:
        st.caption("Filtre objet: " + SUBJECT_MONTHLY_FILTER)

    if "monthly_email_list" in st.session_state:
        with st.expander("üì¨ E-mails trouv√©s (cliquer pour voir / cocher √† inclure)", expanded=True):
            sel = st.session_state.get("monthly_selected_ids", set())
            new_sel = set(sel)
            for m in st.session_state["monthly_email_list"]:
                mid = m["message_id"]
                checked = st.checkbox(
                    f"[{m['dt'].strftime('%Y-%m-%d %H:%M')}] {m['subject']}",
                    key=f"mo_mail_{mid}",
                    value=(mid in sel)
                )
                if checked: new_sel.add(mid)
                else:       new_sel.discard(mid)
                with st.expander("Voir l‚Äôaper√ßu", expanded=False):
                    st.write((m["body"] or "")[:1500] + ("..." if len(m["body"])>1500 else ""))
            st.session_state["monthly_selected_ids"] = new_sel

    # Rubriques √† afficher
    st.markdown("**Rubriques √† afficher**")
    cols = st.columns(5); chosen=[]
    for i,f in enumerate(DAILY_FIELDS):
        with cols[i%5]:
            if st.checkbox(f.replace('_',' ').title(), value=True, key=f"m_{f}"): chosen.append(f)
    chosen = chosen or DAILY_FIELDS

    # PDFs optionnels
    st.markdown("**(Optionnel) D√©poser des PDF**")
    pdf_files = st.file_uploader("Glisser-d√©poser des PDF (facultatif)", type=["pdf"], accept_multiple_files=True, key="monthly_pdf")

    if st.button("‚öôÔ∏è G√©n√©rer le Monthly"):
        emails_override = None
        if "monthly_email_list" in st.session_state:
            ids = st.session_state.get("monthly_selected_ids", set())
            emails_override = [m for m in st.session_state["monthly_email_list"] if m["message_id"] in ids]

        docs = collect_sources_for_range(start_dt, end_dt, SUBJECT_MONTHLY_FILTER, pdf_files, emails_override=emails_override)
        if not docs:
            st.warning("Aucune source s√©lectionn√©e/trouv√©e.")
        else:
            # 1 seul appel de tri
            categorized = classify_all_at_once(docs)
            monthly_input = "\n\n".join(
                f"## {k}\n{categorized[k]}" for k in DAILY_FIELDS if categorized.get(k, "").strip()
            )
            # 1 appel de synth√®se mensuelle
            llm = synth_llm_monthly(monthly_input)
            fields = {k: dedupe_bullets(llm.get(k,"")) for k in DAILY_FIELDS}
            global_summary = dedupe_bullets(llm.get("global_summary",""))
            month_key = f"{int(y):04d}-{int(m):02d}"
            payload = {
                "month": month_key,
                "fields": fields,
                "global_summary": global_summary,
                "sources":[{"title":d["title"],"kind":d["kind"],"dt":d["dt"].isoformat()} for d in docs]
            }
            st.session_state["monthly_preview"]=payload
            st.success("Monthly g√©n√©r√©.")

    if "monthly_preview" in st.session_state:
        mo = st.session_state["monthly_preview"]
        st.markdown(f"### Monthly : {mo['month']}")
        with st.expander("R√©sum√© global", expanded=True):
            st.markdown(md_block(mo.get("global_summary","")))
        for k in DAILY_FIELDS:
            if k in chosen:
                with st.expander(k.replace("_"," ").title(), expanded=False):
                    st.markdown(md_block(mo["fields"].get(k,"")))
        with st.expander("Sources utilis√©es"):
            for s in mo.get("sources", []):
                st.write(f"- [{s['kind']}] {s['dt']} ‚Äî {s['title']}")
        if st.button("üíæ Enregistrer ce Monthly"):
            save_json(monthly_path(mo['month']), mo); st.success(f"Monthly sauvegard√© : {monthly_path(mo['month'])}")
